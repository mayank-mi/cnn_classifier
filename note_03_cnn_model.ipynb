{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKqUlEQVR4nO3cTYjV9R7H8d+5HYwhC6KGsCJsaBGYRFFgLQx7goEgXQS2cFO2qBZBpbiJoIVBEA1muKhVi2bXIqxFSpEEPVCLwspchJQP2EgRmj0wee6qD5e7uM33hzPOHV+v9fnM74/MnDf/hb/BaDQaNQBorf3rXD8AAIuHKAAQogBAiAIAIQoAhCgAEKIAQIgCADGc6wcHg8F8PgcA82wu/1fZmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMTwXD/A+eDKK68sb15++eWus2644YbyZsuWLeXNW2+9Vd4Ai583BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYA4ry/EW7VqVXnz4osvljf33ntveTMYDMqbXrfffnt540K8fpdddlnXbtOmTeXN8ePHy5u1a9eWN1NTU+XNt99+W94w/7wpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCD0Wg0mtMHF/DWzh6XXHJJeTMzM1PeLFu2rLzpcfTo0a7dm2++Wd488cQT5c2ZM2fKm6VobGysvNm/f3/XWRMTE127hXDnnXeWN++///48PAn/y1y+7r0pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMTwXD/Af5ucnOzaTU9Plzc9l9sdPny4vDly5Eh5s2rVqvKmtdYeeuih8ubnn38ub3bv3l3e3HrrreVNa619/fXX5c1nn31W3pw8ebK8+fPPP8ubr776qrxpre9CvF27dpU3Dz/8cHmzfv368saFeIuTNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAGIxGo9GcPjgYlH/4gw8+WN688cYb5U1rre3fv7+8efXVV8ubHTt2lDc9VqxY0bV75ZVXypsNGzZ0nbWY/fXXX+XNwYMHy5sLLrigvBkfHy9vWmvt0ksvLW96Ljtcvnx5eTM7O1ve/PLLL+VNa33/5k8//XR58/rrr5c3i91cvu69KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDEcD5/+L59+8qbTZs2dZ01PT1d3vRcmrZQjh071rX79ddfy5uei8nefffd8uapp54qb1prbWJiorxZs2ZNeXPjjTeWN8Nh/U/o9OnT5U1rra1bt668ueaaa7rOqvr888/Lmy+//LLrrNWrV5c3PZdffvrpp+XNgQMHypvFxpsCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADEYjUajOX1wMJjvZ+E/XHHFFV27Q4cOlTc7d+4sb7Zs2VLesPAuuuii8mZsbKy8OXHiRHnTa3x8vLw5evRoedNzE/DU1FR501pre/fuLW/m+NVd3nhTACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4hUNh8Py5tFHHy1vtm/fXt601try5cvLm5mZmfLm+eefL29eeuml8oalq+d3qLXWNm7cWN6sXLmy66yFct9995U3b7/9dnnjQjwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiPP6QryeS7JeeOGF8uaBBx4ob06dOlXetNbajz/+WN789NNP5c3VV19d3qxYsaK84f/D3XffXd7s2bOn66y9e/eWN6+99lp5c+zYsfLmgw8+KG9aa+3JJ58sb3oumHQhHgAlogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE8Fw/wNly//33lzfT09PlzdjYWHmzb9++8mbt2rXlTWutPfLII+XNHXfcUd5cddVV5Q1L1+bNm8ub77//vuusycnJ8mZ2dra8ue6668qbXkeOHFmws/6JNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAWDIX4m3cuLG86bnc7pNPPilv1q1bV97cdddd5U1rrb333nvlzfbt28ubjz76qLxh6RofHy9vDh482HVWz+V2PSYmJhbknNZaO3To0IKd9U+8KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQS+aW1J4bT3t8/PHH5c2ZM2fKmz179pQ3rbW2Zs2a8ubaa68tb7Zt21besHRdfPHF5c3hw4fn4UnOnpUrVy7YWW5JBWBREgUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAglsyFeMuWLStvvvvuu/Lm2WefLW96DAaDrt3WrVvLm1OnTpU3u3fvLm9YunouxDt58uQ8PMnZc/PNN5c3PX9LrbU2MzPTtZsP3hQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYslciPfOO++UN5OTk+XN6tWry5sPP/ywvOm9eG/Dhg3lzWOPPVbenD59urxh6fr999/Lm8svv7zrrJ7LIm+77bbyZvPmzeXNzp07y5vWWhuNRl27+eBNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACAGoznexNRzCdVCuvDCC8ubb775prwZGxsrb3744Yfy5pZbbilvWmtt165d5c3jjz/edRb87bnnnitvnnnmma6zTpw4Ud4Mh/W7P48fP17e3HTTTeVNa6399ttvXbuquXzde1MAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiCVzIV6P66+/vryZmpoqb/7444/y5osvvihvWuu7mGx2drbrLPhbz/fD+vXru8665557unZVO3bsKG8OHDgwD09y9rgQD4ASUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACI8/qWVIDziVtSASgRBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYjjXD45Go/l8DgAWAW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxL8BccyZbi0vxeUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=10000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (400000, 1, 28, 28)\n",
      "X_test shape: (100000, 1, 28, 28)\n",
      "y_train shape: (400000,)\n",
      "y_test shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=50000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 1,28, 28)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (400000, 1, 28, 28)\n",
      "X_test shape after reshaping: (100000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (100000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (400000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32) #dtype=torch.float32\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKNUlEQVR4nO3cTYhW5R/G8fvYlGQQLoJm0aIEwwwhy032AiaCRRBWBC1atQpCyEycWtQqWtYm2lSQtVFcWZsJKgiiBpqMbKjIVdmLbcrCfAFPu4v4E/yf35meZ8bp81mfi3MW+ny9F95d3/d9A4DW2qql/gAAlg9RACBEAYAQBQBCFAAIUQAgRAGAEAUAYmrUB7uuG+d3ADBmo/xfZScFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBj5Qjzgv+HGG28sbzZv3lzevPnmm+UN4+ekAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBd3/f9SA923bi/Bf4Thv5duuWWW8qbhx56qLx58skny5uff/65vLnmmmvKm9Zau3DhwqAdrY3yc++kAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEBMLfUHwMVs79695c2QW0hba216enrQrurrr78ub+64447yxm2ny5OTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB0fd/3Iz3YdeP+FlhSu3btKm8OHz5c3rz99tvlTWut3XbbbeXNJZdcUt7cdddd5c38/Hx5w+SN8nPvpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQLsRjRVq3bl158+mnn5Y3X3zxRXkz5MK51lp75JFHypsvv/yyvJmbmytvuDi4EA+AElEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYmqpPwD+n9WrV5c3Bw8eLG/OnTtX3jz99NPlzfz8fHnTWmvffvtteXPVVVeVN8eOHStvTp8+Xd6wPDkpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQL8ZiYa6+9dtDulVdeKW82b95c3uzcubO8WVhYKG8++eST8qa11rZs2VLe3HfffeXNo48+Wt5s3769vDlx4kR5w/g5KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQXd/3/UgPdt24v4UlsmpV/d8GMzMz5c0zzzxT3rTW2tmzZ8ub3bt3lzcHDhwob5a7rVu3ljfvv/9+efPCCy+UN88++2x5w+KM8nPvpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQLsSjPf/88+XNkAvx3njjjfKmtdaeeuqp8ubkyZOD3kVrR44cKW/Wr19f3mzYsKG8YXFciAdAiSgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAMbXUH8C/a9u2beXN/v37y5sXX3yxvHniiSfKGybv3XffLW/uvffe8mbt2rXlTWut/frrr4N2jMZJAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACC6vu/7kR7sunF/C3+zZs2aQbvPP/+8vDl//nx5c/PNN5c3Z86cKW+YvB07dpQ3s7Oz5c2tt95a3rTW2scffzxoR2uj/Nw7KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQU0v9Afyzffv2Ddpdd9115c3WrVvLGzeerlwLCwsTec8NN9wwaOeW1PFyUgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIF+ItU7t27Rq0m52dLW/m5uYGvYuV6cSJE+XNb7/9Vt5s3LixvGH8nBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAwoV4EzA9PV3ebNq0adC7Xn311UE7WIzff/+9vLniiivG8CUslpMCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQLgQbwJ27txZ3nRdN+hds7Ozg3awGJdeeml5c/78+TF8CYvlpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQLsSbgPXr15c3p0+fHvSur776atAOFsOFeCuHkwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4ZbUCVi9enV5c+bMmTF8CYyHW1JXDicFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAh3gS4EI+LyapV9X8rDvkzfvbs2fKG8XNSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4k3AuXPnypsrr7xy0Lsuv/zy8ubPP/8c9C5Wpk2bNpU3l112WXlz7Nix8obxc1IAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfiTcChQ4fKmz179gx614MPPljeHDhwYNC7WJl27NhR3vR9X9589NFH5Q3j56QAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEF0/4k1WXdeN+1v4m6NHjw7anTp1qry58847B72L5W/NmjXlzfHjx8ubzz77rLy55557yhsWZ5SfeycFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGJqqT+Af/baa68N2r300kvlzcaNG8ubhYWF8obJe/zxx8ubq6++urx57rnnyhuWJycFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgOj6vu9HerDrxv0t/M3atWsH7X744Yfy5uWXXy5v9u7dW95cf/315U1rrd19993lzYYNG8qbCxculDczMzPlzalTp8qb1lq76aabypv33nuvvPnggw/Km/vvv7+8YfJG+bl3UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIF+KtMG+99VZ5M+TCufn5+fJm+/bt5U1ro13i9b++++678mZ6erq8mZubK2+++eab8qa11h544IHy5pdffilvtm3bVt58//335Q2T50I8AEpEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgX4q0wt99+e3nz4Ycfljd//PFHeTMzM1PetNba4cOHy5sff/yxvHn44YfLm9dff728GfJtrbV29OjR8uaxxx4rb3766afyhouDC/EAKBEFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHBLKm337t3lzTvvvFPeHD9+vLwB/j1uSQWgRBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCEewH+EC/EAKBEFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIKZGfXDEe/MAuIg5KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABB/AZEEjOOj4W6WAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[5]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x00000287FF932A50>, <torch.utils.data.dataloader.DataLoader object at 0x0000028820864470>)\n",
      "Length of train dataloader: 12500 batches of 32\n",
      "Length of test dataloader: 3125 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 2, 4, 9, 0, 9, 3, 4, 1, 7, 5, 1, 8, 2, 3, 8, 2, 2, 9, 1, 9, 3, 9, 0,\n",
       "        0, 9, 2, 8, 3, 1, 4, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "  3%|▎         | 1/30 [00:27<13:06, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.3991 | Test Loss: 0.2465 | Test Acc: 91.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:53<12:31, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.3024 | Test Loss: 0.2112 | Test Acc: 93.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:20<11:58, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.2786 | Test Loss: 0.2038 | Test Acc: 93.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:46<11:28, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.2655 | Test Loss: 0.1975 | Test Acc: 93.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:19<11:59, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.2582 | Test Loss: 0.1932 | Test Acc: 93.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:53<12:17, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.2522 | Test Loss: 0.1904 | Test Acc: 93.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [03:32<12:47, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.2477 | Test Loss: 0.1853 | Test Acc: 94.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [04:15<13:21, 36.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.2439 | Test Loss: 0.1882 | Test Acc: 93.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [04:49<12:30, 35.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.2405 | Test Loss: 0.1847 | Test Acc: 94.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [05:26<12:00, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.2382 | Test Loss: 0.1836 | Test Acc: 94.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [05:58<11:00, 34.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.2368 | Test Loss: 0.1829 | Test Acc: 94.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [06:37<10:52, 36.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.2344 | Test Loss: 0.1799 | Test Acc: 94.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [07:04<09:25, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.2320 | Test Loss: 0.1813 | Test Acc: 94.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [07:30<08:19, 31.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.2298 | Test Loss: 0.1788 | Test Acc: 94.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [08:06<08:06, 32.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.2295 | Test Loss: 0.1790 | Test Acc: 94.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [08:48<08:17, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.2278 | Test Loss: 0.1784 | Test Acc: 94.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [09:31<08:11, 37.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.2281 | Test Loss: 0.1758 | Test Acc: 94.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [10:14<07:51, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.2266 | Test Loss: 0.1747 | Test Acc: 94.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [10:58<07:25, 40.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.2235 | Test Loss: 0.1767 | Test Acc: 94.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [11:40<06:51, 41.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.2237 | Test Loss: 0.1749 | Test Acc: 94.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [12:23<06:14, 41.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.2232 | Test Loss: 0.1739 | Test Acc: 94.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [13:06<05:36, 42.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.2225 | Test Loss: 0.1763 | Test Acc: 94.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [13:49<04:55, 42.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.2222 | Test Loss: 0.1723 | Test Acc: 94.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [14:25<04:03, 40.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.2217 | Test Loss: 0.1723 | Test Acc: 94.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [14:51<03:00, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.2208 | Test Loss: 0.1733 | Test Acc: 94.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [15:23<02:19, 34.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.2184 | Test Loss: 0.1736 | Test Acc: 94.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [16:06<01:51, 37.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.2200 | Test Loss: 0.1711 | Test Acc: 94.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [16:37<01:11, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.2176 | Test Loss: 0.1732 | Test Acc: 94.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [17:03<00:32, 32.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.2183 | Test Loss: 0.1729 | Test Acc: 94.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [17:29<00:00, 35.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.2169 | Test Loss: 0.1708 | Test Acc: 94.45%\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel04(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128*3*3, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features=256, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel04(input_channels=1, output_shape=len(class_dict)) # For grayscale images\n",
    "model_2.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    scheduler.step(test_loss)  # Update the scheduler\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DoodleClassModel04',\n",
       " 'model_loss': 0.1708349734544754,\n",
       " 'model_acc': 94.455}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Move data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Move model to the device\n",
    "model_2.to(device)\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[6].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape the image to (1, 1, 28, 28) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    \n",
    "    # Check if CUDA (GPU) is available and move tensor to CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_apple\n",
      "Predicted class index: 5\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model_2(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_2.state_dict(), './models/cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
