{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.1-cp312-cp312-win_amd64.whl.metadata (165 kB)\n",
      "     ---------------------------------------- 0.0/165.9 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 112.6/165.9 kB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 165.9/165.9 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in .\\lib\\site-packages (from matplotlib) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in .\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in .\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.1-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/8.0 MB 6.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/8.0 MB 5.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.8/8.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.2/8.0 MB 7.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.6/8.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.2/8.0 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.8/8.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 3.4/8.0 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.0/8.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.4/8.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.0/8.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.6/8.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/8.0 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/8.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.4/8.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.0/8.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp312-cp312-win_amd64.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 189.9/189.9 kB ? eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.2 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp312-cp312-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.0/56.0 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.6 MB 12.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.6 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.6 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.0/2.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.2/103.2 kB ? eta 0:00:00\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1 pillow-10.4.0 pyparsing-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134801, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'C:/demo/classfier_cnn/data\\The Eiffel Tower.npy' is the correct path to your .npy file\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/The Eiffel Tower.npy')\n",
    "\n",
    "# Get the shape of the array\n",
    "shape_of_array = np.shape(array_data)\n",
    "\n",
    "print(shape_of_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape_of_first_5000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m first_5000_samples \u001b[38;5;241m=\u001b[39m array_data[:\u001b[38;5;241m5000\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the shape of the sliced array\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# shape_of_first_5000 = np.shape(first_5000_samples)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mshape_of_first_5000\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shape_of_first_5000' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the array from the .npy file\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/The Eiffel Tower.npy')\n",
    "\n",
    "# Get the first 5000 samples\n",
    "first_5000_samples = array_data[:5000]\n",
    "\n",
    "# Get the shape of the sliced array\n",
    "# shape_of_first_5000 = np.shape(first_5000_samples)\n",
    "\n",
    "print(shape_of_first_5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[6].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[6].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (40000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming X_train is already defined with shape (40000, 28, 28, 1)\n",
    "# X_train = np.reshape(X_train, (40000, 1, 28, 28))\n",
    "# X_test = np.reshape(X_test, (10000, 1, 28, 28))\n",
    "\n",
    "# # Print the shape after reshaping\n",
    "# print(\"X_train shape after reshaping:\", X_train.shape)\n",
    "# print(\"X_test shape after reshaping:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch\n",
    "X_test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (40000, 1, 28, 28)\n",
      "X_test shape after reshaping: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (10000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (40000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "first_image = X_test_reshaped.reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "print(first_image.shape)\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "# plt.title(class_dict[y_test[rank]])\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rank = 9\n",
    "\n",
    "first_image = X_test[rank].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "print(first_image.shape)\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.title(class_dict[y_test[rank]])\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[1]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x000001834E8732F0>, <torch.utils.data.dataloader.DataLoader object at 0x000001834D5A7830>)\n",
      "Length of train dataloader: 1250 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 6, 5, 0, 8, 9, 0, 1, 8, 5, 2, 5, 7, 7, 1, 9, 1, 0, 4, 4, 3, 9, 9, 8,\n",
       "        3, 4, 4, 5, 2, 3, 2, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([28, 28, 1])\n",
      "Label: 2, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAActklEQVR4nO3ceXQV5f3H8c9NwOQCJUGIgCxBjQUUEAunyCJLQImCQasERCQREEUFLHWFIyGxslhlEUEsVSghWBYXWimg0GCsQosLqKQUIYjWBSSyqBDAZH5/9JdvuSRInjGb+H6dwzneYT7zPJlD7ufOnfEJeJ7nCQAASWGVPQEAQNVBKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQyn8AJs2bVKnTp1Us2ZNBQIBbd68udTZQCCgiRMn2usFCxYoEAjoo48+KvN5nukmTpyoQCCgffv2nXbfZs2aKSUlpfwnVUlSUlJUq1atyp7GKQUCAd11111ldrz169crEAho/fr1ti0lJUXNmjUrszF+aqpV9gR+rI4fP67+/fsrMjJS06dPV40aNRQbG1vZ04KjnJwcLV26lDcS4P9RCj7t3LlTu3fv1rx58zR8+PDKng5K6d///rfCwv53gZyTk6O0tDR1796dUgBEKfi2d+9eSVJ0dHTlTgROIiIiKnsKQJXGPQUfUlJS1K1bN0lS//79FQgE1L17d/tT0v7l8Sm06Lv0HTt2KCUlRdHR0YqKitItt9yiw4cP234fffSRAoGAFixYUOwYJ9/bKDrm9u3bNXjwYEVFRSkmJkYPPfSQPM/TJ598on79+ql27dpq0KCBHn/88ZDjFX3Hu2TJEo0bN04NGjRQzZo1lZiYqE8++cT2S01NVfXq1fXll18Wm9OIESMUHR2t/Px8p/Oxb98+JSUlqXbt2qpbt67GjBlT7Bgn3lNYsGCB+vfvL0nq0aOHAoFAyPfTzZo1U9++fbV+/Xq1b99ewWBQrVu3tr9/4YUX1Lp1a0VGRqpdu3Z69913Q8Z67733lJKSovPPP1+RkZFq0KCBhg4dqry8vJD9is75tm3bTjv/0srNzVXv3r1Vs2ZNnXvuuUpPT9fJCyI/9thj6tSpk+rWratgMKh27dpp+fLlxY716quvqkuXLoqOjlatWrXUvHlzjRs3LmSfo0ePKjU1VXFxcYqIiFCTJk1033336ejRoyXOLzMzU82bN7dzl52dHfL3u3fv1h133KHmzZsrGAyqbt266t+/P/fcKgCl4MNtt91mvxSjR49WRkaGxo8fX2nzSUpK0tdff63JkycrKSlJCxYsUFpa2g865oABA1RYWKgpU6aoQ4cO+u1vf6sZM2boiiuuUKNGjTR16lTFxcXpnnvuKfYLLUmPPPKIVq5cqfvvv1+jR4/Wq6++ql69eunIkSOSpJtvvlnfffedlixZEpI7duyYli9fruuvv16RkZFOc05KSlJ+fr4mT56sq6++Wk888YRGjBhxyv27du2q0aNHS5LGjRunjIwMZWRkqGXLlrbPjh07NGjQIF1zzTWaPHmy9u/fr2uuuUaZmZn69a9/rcGDBystLU07d+5UUlKSCgsLLfvqq68qNzdXt9xyi2bNmqWBAwfqT3/6k66++upib9B+5n8qBQUFSkhIUP369fXoo4+qXbt2Sk1NVWpqash+M2fO1KWXXqr09HRNmjRJ1apVU//+/bVy5UrbZ+vWrerbt6+OHj2q9PR0Pf7440pMTNQbb7xh+xQWFioxMVGPPfaYrrnmGs2aNUvXXnutpk+frgEDBhSb32uvvaa7775bgwcPVnp6uvLy8pSQkKAPPvjA9tm0aZPefPNNDRw4UE888YRuv/12rVu3Tt27dw/5wINy4MGXrKwsT5K3bNky29atWzevW7duxfZNTk72YmNjQ7ZJ8lJTU+31/PnzPUnerl27Sj2H1NRUT5I3dOjQkO3XXXedV7duXXu9a9cuT5I3f/78Ysc4eR5FxxwxYoRt++6777zGjRt7gUDAmzJlim3fv3+/FwwGveTkZNtWdF4aNWrkHTp0yLYvXbrUk+TNnDnTtnXs2NHr0KFDyHxeeOEFT5KXlZVV2tNgc05MTAzZfscdd3iSvC1btti22NjYkPkuW7bslOPFxsZ6krw333zTtq1Zs8aT5AWDQW/37t22/emnny52nMOHDxc75nPPPedJ8rKzs33N/3SSk5M9Sd6oUaNsW2FhodenTx/vrLPO8r788stTzu/YsWNeq1atvPj4eNs2ffp0T1JI7mQZGRleWFiY9/rrr4dsnzt3rifJe+ONN2ybJE+S99Zbb9m23bt3e5GRkd511113yrl5nudt2LDBk+QtXLjQthX9ezvxvJf0+4bS40rhDHD77beHvL788suVl5enQ4cO+T7miTfPw8PD1b59e3mep2HDhtn26OhoNW/eXLm5ucXyQ4YM0c9+9jN7fcMNN6hhw4b661//GrLPP/7xD+3cudO2ZWZmqkmTJvb1nIs777wz5PWoUaMkKWRMVxdddJE6duxorzt06CBJio+PV9OmTYttP/FcBINB++/8/Hzt27dPl112mSTpnXfeKdf5n/jYZ9FjoMeOHdPatWtLnN/+/ft18OBBXX755SFzK7pntmLFipCroBMtW7ZMLVu2VIsWLbRv3z77Ex8fL0nKysoK2b9jx45q166dvW7atKn69eunNWvWqKCgoNjcjh8/rry8PMXFxSk6OrrEc4eyQymcAU58c5KkOnXqSPrvL3pZHTMqKkqRkZGqV69ese0ljXPhhReGvA4EAoqLiwv5TnjAgAGKiIhQZmamJOngwYN6+eWXddNNNykQCDjP+eQxL7jgAoWFhf2g76FLOg+S1KRJkxK3n3guvvrqK40ZM0b169dXMBhUTEyMzjvvPEn//VnLa/5hYWE6//zzQ7b9/Oc/l6SQY7388su67LLLFBkZqbPPPlsxMTF66qmnQuY2YMAAde7cWcOHD1f9+vU1cOBALV26NKQgPvzwQ23dulUxMTEhf4rGLHoo41Q/Z9H8Dh8+bPeYjhw5ogkTJqhJkyaKiIhQvXr1FBMTowMHDpR47lB2ePqoDAUCgRK/Ky769FNewsPDS9xeNJdTvcF+37xKOubpxnFVp04d9e3bV5mZmZowYYKWL1+uo0ePavDgwb6OdzI/xXKyU/3MpTkXSUlJevPNN3Xvvfeqbdu2qlWrlgoLC5WQkHDKT90nKov5n8rrr7+uxMREde3aVXPmzFHDhg1VvXp1zZ8/X4sXL7b9gsGgsrOzlZWVpZUrV2r16tVasmSJ4uPj9corryg8PFyFhYVq3bq1pk2bVuJYJxdoaYwaNUrz58/X3XffrY4dOyoqKkqBQEADBw4s1bmDf5RCGapTp06JX6Xs3r27EmbzP0VXDgcOHAjZXp7z+vDDD0Nee56nHTt2qE2bNiHbhwwZon79+mnTpk3KzMzUpZdeqosvvtj3mEWfxKX/3iQuLCz83ie/yuuNd//+/Vq3bp3S0tI0YcKEkDmeip/5l6SwsFC5ubn2SV2Stm/fLkl2rOeff16RkZFas2ZNyGO68+fPL3a8sLAw9ezZUz179tS0adM0adIkjR8/XllZWerVq5cuuOACbdmyRT179izV+SzpHGzfvl01atRQTEyMJGn58uVKTk4OebotPz+/2L9hlD2+PipDF1xwgbZt2xbymOWWLVtCntSoDLVr11a9evWKPSU0Z86cchtz4cKF+vrrr+318uXL9fnnn+uqq64K2e+qq65SvXr1NHXqVL322ms/6Cph9uzZIa9nzZplY5xKzZo1JRUvzB+q6Eri5KuoGTNmnDLjZ/6n8uSTT9p/e56nJ598UtWrV1fPnj1tfoFAIORq8aOPPtJLL70Ucpyvvvqq2LHbtm0rSfa4aVJSkj799FPNmzev2L5HjhzRt99+G7Jtw4YNIfcFPvnkE61YsUJXXnmlnbfw8PBi527WrFnlftUNrhTK1NChQzVt2jT17t1bw4YN0969ezV37lxdfPHFP+imb1kYPny4pkyZouHDh6t9+/bKzs62T4/l4eyzz1aXLl10yy23aM+ePZoxY4bi4uJ06623huxXvXp1DRw4UE8++aTCw8N14403+h5z165dSkxMVEJCgjZs2KBFixZp0KBBuuSSS06Zadu2rcLDwzV16lQdPHhQERERio+P1znnnON7HtJ/i7hr16569NFHdfz4cTVq1EivvPKKdu3aVabzL0lkZKRWr16t5ORkdejQQatWrdLKlSs1btw4+yTep08fTZs2TQkJCRo0aJD27t2r2bNnKy4uTu+9954dKz09XdnZ2erTp49iY2O1d+9ezZkzR40bN1aXLl0k/ffx4qVLl+r2229XVlaWOnfurIKCAm3btk1Lly7VmjVr1L59eztmq1at1Lt3b40ePVoRERH24eTEx6j79u2rjIwMRUVF6aKLLtKGDRu0du1a1a1b1+lcwB1XCmWoZcuWWrhwoQ4ePKixY8fqz3/+szIyMvSLX/yisqemCRMmaNiwYVq+fLnuu+8+FRQUaNWqVeU23rhx49SnTx9NnjxZM2fOVM+ePbVu3TrVqFGj2L5DhgyRJPXs2VMNGzb0PeaSJUsUERGhBx54QCtXrtRdd92lZ5555nszDRo00Ny5c7V3714NGzZMN954o3JycnzP4USLFy9W7969NXv2bD344IOqXr36955zP/MvSXh4uFavXq0vvvhC9957rzZt2qTU1FQ9/PDDtk98fLyeeeYZffHFF7r77rv13HPPaerUqbruuutCjpWYmKimTZvq2Wef1Z133qnZs2era9eu+tvf/mY318PCwvTSSy9pypQpev/993XPPfcoLS1NmzZt0pgxY0K+xpKkbt26acaMGcrIyNCECRN09tlna9WqVSFfLc6cOVNDhgxRZmamfvOb3+jzzz/X2rVrq/Rif2eKgOf3LiFQgvXr16tHjx5atmyZbrjhhlJltmzZorZt22rhwoW6+eaby3mGVc/EiROVlpamL7/8stjTXUBF40oBlW7evHmqVauWfvWrX1X2VICfPO4pVEHffPONvvnmm+/dJyYm5pSPRf5Y/OUvf1FOTo5+//vf66677rKbvkV+KuehNA4ePGhLhJxKgwYNKmg2OJNRClXQY489dtq1i3bt2vWjX+p51KhR2rNnj66++uoSf96fynkojTFjxuiPf/zj9+7DN8EoC9xTqIJyc3NL/P8dTtSlSxfnBeN+bDgP/5OTk6PPPvvse/fp1atXBc0GZzJKAQBguNEMADClvqdQnuuwAADKX2m+GOJKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgqlX2BFD5AoGAcyY+Pt45c/311ztnJKlRo0bOmejoaOfMW2+95ZxZtmyZc2bjxo3OGaCicKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADABz/O8Uu3oYyVN+BcVFeUrl5yc7JwZOXKkc6ZFixbOmby8POeMJO3YscM5c/jwYedM586dnTNnnXWWcyY9Pd05I0lPPfWUc+bAgQPOmfz8fOcMfhxK83bPlQIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwLIhXAW688UbnjJ/FzySpdu3azpn169c7Z+bMmeOcWbFihXNGkoLBoHNmyZIlzpmEhATnzKFDh5wzERERzhm/uWPHjjlnXnzxRefMjBkznDMbN250zuCHYUE8AIATSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAOYnvSCen59p5syZzplRo0Y5Z9atW+eckaTRo0c7Z3JycpwzkydPds7Ur1/fOSNJV111lXOmXr16zpl//vOfzplOnTo5Z9auXeuckaTFixc7Zy688ELnzLBhw5wzMTExzpmJEyc6ZyTp4Ycfds6U8m3ujMeCeAAAJ5QCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAADMT3pBvN/97nfOmXvuucc542cBL7+LhRUWFvrKuXr++eedMz169PA1Vp06dZwzR48edc6kpaU5Zz7++GPnTGZmpnOmIkVERDhnpk+f7pwZOXKkc0aS5s+f75y59dZbnTMFBQXOmaqOBfEAAE4oBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAmDNmQbzBgwc7ZzIyMpwzU6dOdc488MADzpmKFAwGnTNNmzZ1zmzevNk5I0mLFi1yzvhZAA0V69577/WV8/M7OHnyZOfM+PHjnTNVHQviAQCcUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAVLlVUhs1auQr9/777ztn3nnnHefMFVdc4Zwp5SkuE/fff79zZtKkSc6Zo0ePOmfCwvx9BrnpppucM88//7yvsVD1PfHEE86ZO++80zkTHx/vnHnttdecMxWJVVIBAE4oBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAmGqVPYGTjR071lcuGAw6Z4YOHeqcqcjF7fzYv3+/c2bjxo3OmU6dOjln/vOf/zhnJOncc8/1lcOZ6b777nPOdO/e3TmTkZHhnLnkkkucM5K/39vywpUCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMOW6IF5kZKRzJiUlxddYS5cudc58/PHHvsaqymJjY50zv/zlL8thJsU1btzYV65FixZlPBP8mOXn5ztnBg0a5Jx5++23nTMPPfSQc0byvxBoeeBKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAAJiA53leqXYMBJwP3qVLF+fM66+/7pyRpCuuuMI5s3btWl9jnWlycnKcM7Vq1XLOTJ8+3TkjSZs3b3bOZGVl+RoLKDJ37lznzJAhQ3yN1aRJE+dMXl6ec6Y0b/dcKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAATLXyPPh5551XnocPsX379gob60wTGxvrnKlRo4ZzJjU11TkjSe+++65zpk2bNs6ZmTNnOmdw5lq0aJFz5rbbbvM11sUXX+ycyc7O9jXW6XClAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAEy5LojXqFEj54zneb7G+vTTT33lIG3dutU5c/jwYefM5s2bnTOS1KpVK+dMRS7GiIrVsGFD58yePXucM/v27XPO+FW3bt0KG+t0uFIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAplwXxAsPDy/Pw4coKCiosLGqss6dOztnWrZs6Zzxs3DhmDFjnDOStGXLFl85+FO/fn3njJ8F52rVquWckaSdO3c6Z0aOHOmcWbVqlXPGLxbEAwBUSZQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAABMuS6Id+zYMedMIBDwNVbNmjWdM99++62vsaqyXbt2OWc2btzonOnVq5dzJjk52TkjSWPHjvWVg3TOOec4Zz777DPnTGpqqnNmw4YNzhlJCgaDzpm8vDznzDfffOOc8Ss6OrrCxjodrhQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAKdcF8d54443yPHwIPwu0rVixohxmUrn8LGZ25ZVXOmeGDh3qnHnxxRedM/hh9u7d65x5+umnnTPp6enOmW3btjlnJOngwYPOmYyMDOfMvHnznDN+ffrppxU21ulwpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMAHP87xS7RgIOB88LMy9cz7//HPnjCRlZ2c7Z/r37+9rLOBMVq2a++LJ7777rnOmVatWzhlJ+vrrr50zX331lXPGz2qxkyZNcs5IUuvWrZ0zH3zwgXOmNG/3XCkAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAU64L4vnx4IMP+so98sgjzpmuXbs6Z/7+9787Z4Afk2bNmjln/vWvfzlnIiIinDOSlJ+f75w5dOiQc6awsNA5c+TIEeeMJMXFxTlnSvnW7ZzhSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAACYKrcgnt9FsrZu3eqc+fbbb50znTp1qpBxgMrSvHlz58wf/vAH50yLFi2cM5IUHR3tnDl+/Lhzxs970bhx45wzkjR16lRfOVcsiAcAcEIpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAVLkF8fzq0aOHc2bNmjXOmXXr1jlnEhMTnTN+FvACfkyuvfZaX7kXX3zRObN7927nTGRkpHPG7yJ/Bw4c8JVzxYJ4AAAnlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAEy1yp5AWcnKynLOJCcnO2cWLVrknFm8eLFzxs/cJOnw4cO+ckBFq127doWNFRsb65x5++23nTMVtbBdeeJKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgAp7neaXaMRAo77n8KIwYMcI5M3v2bOfM+++/75yRpKSkJOfMjh07fI0FFOnSpYtz5uWXX/Y1VlRUlHMmNzfXOdOpUyfnzJ49e5wzFak0b/dcKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAADDgngVoEePHs6ZJUuW+BrLz2JhzzzzjHNmxowZzpnt27c7Z/DDNG7c2DkzfPhw58z48eOdM8ePH3fOSNLHH3/snGnTpo1z5tixY86Zqo4F8QAATigFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGAoBQCAoRQAAIZSAAAYFsSrourVq+cr9+CDDzpnRo4c6ZwJBoPOmT179jhnJGnTpk3OmSNHjvgaqyq78MILnTNt27Yt+4mU4Nlnn3XO9OvXz9dYY8eOdc4sXLjQ11hnGhbEAwA4oRQAAIZSAAAYSgEAYCgFAIChFAAAhlIAABhKAQBgKAUAgKEUAACGUgAAGEoBAGBYEA+qX7++cyYhIcE507lzZ+eMJLVp08Y5U61aNV9jVWV+FhRcvXq1c2blypXOmdzcXOdMeHi4c0aSCgoKfOXAgngAAEeUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCskgoAPxGskgoAcEIpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAAylAAAwlAIAwFAKAABDKQAADKUAADCUAgDAUAoAAFOttDt6nlee8wAAVAFcKQAADKUAADCUAgDAUAoAAEMpAAAMpQAAMJQCAMBQCgAAQykAAMz/AUJEh/4+Ep+2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(452)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "# print(label,'pp')\n",
    "plt.title(class_dict[label.item()])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([28, 28, 1]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([28, 28]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class doodle_class(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_0 = doodle_class(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_dict) # one for every class\n",
    ")\n",
    "model_0.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 1/10 [00:01<00:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.01623 | Test loss: 0.94903, Test acc: 69.68%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 2/10 [00:02<00:09,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90624 | Test loss: 0.95613, Test acc: 69.19%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 3/10 [00:03<00:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88992 | Test loss: 0.95127, Test acc: 70.47%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 4/10 [00:04<00:06,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88244 | Test loss: 0.92794, Test acc: 70.78%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 5/10 [00:05<00:05,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87614 | Test loss: 0.94376, Test acc: 70.46%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 6/10 [00:06<00:04,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87389 | Test loss: 0.93913, Test acc: 70.86%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 7/10 [00:08<00:03,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87162 | Test loss: 0.94464, Test acc: 70.39%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 8/10 [00:09<00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86889 | Test loss: 0.96840, Test acc: 70.28%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 9/10 [00:10<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86419 | Test loss: 0.97103, Test acc: 69.54%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:11<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86321 | Test loss: 0.94318, Test acc: 70.62%\n",
      "\n",
      "Train time on cpu: 11.534 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 10\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
       "        [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
       "        [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 15:41:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   48C    P8              14W / 170W |    871MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1464    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      2624    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      6064    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      6452    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      6456    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8512    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      8784    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     10380    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13828    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16956    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     17244    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17672    C+G   ...s\\mattermost-desktop\\Mattermost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'doodle_class',\n",
       " 'model_loss': 0.9431753158569336,\n",
       " 'model_acc': 70.61701277955271}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy')\n",
    "\n",
    "# Extract the first image\n",
    "input_tensor = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "print(input_tensor.shape)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), 'mod01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = input_tensor.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 28, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape#[0]#[176]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = torch.tensor(dat, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = array_data[11].reshape(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[66].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_bat\n",
      "Predicted class index: 6\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model_0(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNNModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# PyTorch expects (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert labels to tensor\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data), labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume data and labels are already loaded as numpy arrays\n",
    "# data: shape (num_samples, 28, 28)\n",
    "# labels: shape (num_samples,)\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype('float32') / 255.0\n",
    "data = data.reshape(-1, 1, 28, 28)  # PyTorch expects (batch_size, channels, height, width)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.from_numpy(labels).long()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(torch.from_numpy(data), labels)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader and print the shapes and some values\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch image shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch labels shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Dummy data for demonstration purposes\n",
    "num_samples = 10000\n",
    "data = np.random.rand(num_samples, 28, 28).astype('float32')\n",
    "labels = np.random.randint(0, 10, num_samples)\n",
    "\n",
    "# Define any transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader and print the shapes and some values\n",
    "for images, labels in data_loader:\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Sample image data: {images[0]}\")\n",
    "    print(f\"Sample label: {labels[0]}\")\n",
    "    break  # Just to print the first batch and stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the inputs and labels\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a custom dataset class for our data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define a CNN model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(30*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 30*4*4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create a dataset and data loader for our data\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = Net()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss at each batch\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the inputs and labels\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        images, labels = images.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
