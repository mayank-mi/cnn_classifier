{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=10000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (400000, 28, 28, 1)\n",
      "X_test shape: (100000, 28, 28, 1)\n",
      "y_train shape: (400000,)\n",
      "y_test shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=50000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (400000, 1, 28, 28)\n",
      "X_test shape after reshaping: (100000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (100000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (400000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32) #dtype=torch.float32\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[1]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x0000017ABED5AAE0>, <torch.utils.data.dataloader.DataLoader object at 0x0000017ABED54FE0>)\n",
      "Length of train dataloader: 12500 batches of 32\n",
      "Length of test dataloader: 3125 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7, 1, 1, 5, 0, 7, 0, 2, 4, 4, 8, 8, 3, 1, 2, 4, 9, 3, 6, 9, 1, 2, 3,\n",
       "        7, 6, 8, 3, 5, 4, 9, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([28, 28, 1])\n",
      "Label: 0, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(452)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "# print(label,'pp')\n",
    "plt.title(class_dict[label.item()])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([28, 28, 1]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([28, 28]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class doodle_class_model_02(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class_model_02(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_2 = doodle_class_model_02(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_dict) # one for every class\n",
    ")\n",
    "model_2.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:21,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.70738 | Test loss: 1.33266, Test acc: 59.82%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:20,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.19682 | Test loss: 1.15681, Test acc: 63.11%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.09591 | Test loss: 1.10516, Test acc: 64.55%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:17,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.05320 | Test loss: 1.07620, Test acc: 64.85%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:17,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.02609 | Test loss: 1.05690, Test acc: 65.37%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:04<00:17,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.00734 | Test loss: 1.04347, Test acc: 65.94%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.99291 | Test loss: 1.03770, Test acc: 66.29%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.98093 | Test loss: 1.02566, Test acc: 66.41%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:06<00:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.97024 | Test loss: 1.02264, Test acc: 66.08%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:07<00:14,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.96078 | Test loss: 1.01385, Test acc: 66.47%\n",
      "\n",
      "Epoch: 10\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:07<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.95356 | Test loss: 1.00486, Test acc: 66.98%\n",
      "\n",
      "Epoch: 11\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:08<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.94648 | Test loss: 0.99812, Test acc: 67.45%\n",
      "\n",
      "Epoch: 12\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:09<00:12,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.93951 | Test loss: 0.99558, Test acc: 67.56%\n",
      "\n",
      "Epoch: 13\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:10<00:11,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.93393 | Test loss: 0.99126, Test acc: 67.10%\n",
      "\n",
      "Epoch: 14\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:10<00:11,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.92851 | Test loss: 0.98647, Test acc: 67.77%\n",
      "\n",
      "Epoch: 15\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:11<00:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.92348 | Test loss: 0.98517, Test acc: 67.83%\n",
      "\n",
      "Epoch: 16\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:12<00:09,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.91871 | Test loss: 0.98219, Test acc: 67.73%\n",
      "\n",
      "Epoch: 17\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:13<00:08,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.91354 | Test loss: 0.98399, Test acc: 67.65%\n",
      "\n",
      "Epoch: 18\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:13<00:08,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.90948 | Test loss: 0.97802, Test acc: 68.06%\n",
      "\n",
      "Epoch: 19\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:14<00:07,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90528 | Test loss: 0.97421, Test acc: 67.94%\n",
      "\n",
      "Epoch: 20\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:15<00:06,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90135 | Test loss: 0.97211, Test acc: 68.03%\n",
      "\n",
      "Epoch: 21\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:15<00:05,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.89732 | Test loss: 0.96646, Test acc: 68.23%\n",
      "\n",
      "Epoch: 22\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:16<00:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.89328 | Test loss: 0.97419, Test acc: 67.83%\n",
      "\n",
      "Epoch: 23\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:17<00:04,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88941 | Test loss: 0.96988, Test acc: 67.93%\n",
      "\n",
      "Epoch: 24\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:18<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.88651 | Test loss: 0.97250, Test acc: 67.30%\n",
      "\n",
      "Epoch: 25\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:18<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88350 | Test loss: 0.95762, Test acc: 68.65%\n",
      "\n",
      "Epoch: 26\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:19<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.87968 | Test loss: 0.97616, Test acc: 67.29%\n",
      "\n",
      "Epoch: 27\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:20<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87655 | Test loss: 0.95403, Test acc: 68.66%\n",
      "\n",
      "Epoch: 28\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:21<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.87328 | Test loss: 0.95392, Test acc: 68.40%\n",
      "\n",
      "Epoch: 29\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:21<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87060 | Test loss: 0.95269, Test acc: 68.38%\n",
      "\n",
      "Train time on cpu: 21.941 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 30\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_2.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_2(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_2(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_2.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
       "        [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
       "        [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'doodle_class_model_02',\n",
       " 'model_loss': 0.9526905417442322,\n",
       " 'model_acc': 68.3805910543131}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:03<01:27,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.0201 | Test Loss: 0.7484 | Test Acc: 76.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:06<01:24,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.8631 | Test Loss: 0.6945 | Test Acc: 78.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:09<01:23,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.8121 | Test Loss: 0.6578 | Test Acc: 78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:12<01:19,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.7771 | Test Loss: 0.6329 | Test Acc: 79.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:15<01:17,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.7551 | Test Loss: 0.6193 | Test Acc: 79.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:18<01:15,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.7274 | Test Loss: 0.6002 | Test Acc: 80.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:21<01:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.7086 | Test Loss: 0.5871 | Test Acc: 81.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:24<01:09,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.7037 | Test Loss: 0.5855 | Test Acc: 81.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:28<01:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.6903 | Test Loss: 0.5785 | Test Acc: 81.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:31<01:03,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.6754 | Test Loss: 0.5707 | Test Acc: 81.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:34<01:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.6367 | Test Loss: 0.5565 | Test Acc: 82.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:37<00:56,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.6238 | Test Loss: 0.5548 | Test Acc: 82.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:40<00:53,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.6236 | Test Loss: 0.5492 | Test Acc: 82.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:43<00:49,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.6188 | Test Loss: 0.5441 | Test Acc: 82.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:46<00:46,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.6094 | Test Loss: 0.5413 | Test Acc: 82.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:49<00:43,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.6064 | Test Loss: 0.5413 | Test Acc: 82.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:53<00:40,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.6048 | Test Loss: 0.5419 | Test Acc: 82.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:56<00:37,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.6000 | Test Loss: 0.5379 | Test Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:59<00:34,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.5948 | Test Loss: 0.5361 | Test Acc: 82.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [01:02<00:31,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.5984 | Test Loss: 0.5346 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [01:05<00:27,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.5859 | Test Loss: 0.5369 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [01:08<00:25,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.5886 | Test Loss: 0.5356 | Test Acc: 82.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [01:11<00:22,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.5884 | Test Loss: 0.5368 | Test Acc: 82.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [01:15<00:18,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.5846 | Test Loss: 0.5360 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [01:18<00:15,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.5860 | Test Loss: 0.5358 | Test Acc: 82.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [01:21<00:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.5882 | Test Loss: 0.5338 | Test Acc: 82.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [01:24<00:09,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.5889 | Test Loss: 0.5353 | Test Acc: 82.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [01:27<00:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.5877 | Test Loss: 0.5350 | Test Acc: 82.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [01:30<00:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.5860 | Test Loss: 0.5356 | Test Acc: 82.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:33<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.5833 | Test Loss: 0.5328 | Test Acc: 82.75%\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units*2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel(input_shape=784, hidden_units=128, output_shape=len(class_dict))\n",
    "model_2.to(\"cpu\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DoodleClassModel',\n",
       " 'model_loss': 0.532804548740387,\n",
       " 'model_acc': 82.74760383386581}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), 'moded02.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0195,  0.0034,  0.0302,  ..., -0.0030, -0.0317,  0.0128],\n",
       "        [-0.0107,  0.0221, -0.0158,  ..., -0.0121,  0.0042,  0.0318],\n",
       "        [-0.0106,  0.0342,  0.0240,  ...,  0.0091,  0.0174,  0.0041]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), 'mod01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('C:/test001/ttt/bat.jpg')\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Optional: to hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('C:/test001/ttt/bat.jpg')\n",
    "\n",
    "# Resize the image to 28x28\n",
    "resized_image = image.resize((28, 28))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image_array = np.array(resized_image)\n",
    "\n",
    "# Ensure the image has the shape (28, 28, 1)\n",
    "# For grayscale, add a new axis to make it (28, 28, 1)\n",
    "image_array = np.expand_dims(image_array, axis=-1)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_array.squeeze(), cmap='gray')\n",
    "plt.axis('off')  # Optional: to hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[6].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_airplane\n",
      "Predicted class index: 8\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model_2(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    \n",
    "    # Check if CUDA (GPU) is available and move tensor to CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:06<03:00,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.9520 | Test Loss: 0.6783 | Test Acc: 78.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:12<02:54,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.8144 | Test Loss: 0.6219 | Test Acc: 79.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:19<02:52,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.7732 | Test Loss: 0.5890 | Test Acc: 81.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:25<02:45,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.7448 | Test Loss: 0.5762 | Test Acc: 81.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:31<02:38,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.7228 | Test Loss: 0.5520 | Test Acc: 82.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:38<02:32,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.7066 | Test Loss: 0.5426 | Test Acc: 82.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:44<02:26,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.6964 | Test Loss: 0.5346 | Test Acc: 82.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:50<02:19,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.6838 | Test Loss: 0.5311 | Test Acc: 82.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:57<02:17,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.6738 | Test Loss: 0.5173 | Test Acc: 83.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [01:07<02:31,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.6646 | Test Loss: 0.5138 | Test Acc: 83.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [01:18<02:44,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.6326 | Test Loss: 0.4990 | Test Acc: 83.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:30<02:50,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.6265 | Test Loss: 0.4970 | Test Acc: 84.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:41<02:49,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.6169 | Test Loss: 0.4955 | Test Acc: 83.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:52<02:45, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.6197 | Test Loss: 0.4914 | Test Acc: 84.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [02:03<02:38, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.6144 | Test Loss: 0.4916 | Test Acc: 84.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [02:14<02:30, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.6110 | Test Loss: 0.4975 | Test Acc: 84.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [02:25<02:21, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.6076 | Test Loss: 0.4882 | Test Acc: 84.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [02:32<01:54,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.6052 | Test Loss: 0.4920 | Test Acc: 84.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [02:38<01:34,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.6008 | Test Loss: 0.4882 | Test Acc: 84.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [02:45<01:19,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.5998 | Test Loss: 0.4843 | Test Acc: 84.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [02:51<01:08,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.6001 | Test Loss: 0.4882 | Test Acc: 84.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [02:58<00:57,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.5965 | Test Loss: 0.4838 | Test Acc: 84.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [03:04<00:49,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.5962 | Test Loss: 0.4838 | Test Acc: 84.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [03:10<00:40,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.5999 | Test Loss: 0.4842 | Test Acc: 84.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [03:17<00:34,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.5949 | Test Loss: 0.4906 | Test Acc: 84.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [03:24<00:27,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.5928 | Test Loss: 0.4827 | Test Acc: 84.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [03:31<00:20,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.5944 | Test Loss: 0.4823 | Test Acc: 84.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [03:37<00:13,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.5983 | Test Loss: 0.4959 | Test Acc: 84.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [03:44<00:06,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.5956 | Test Loss: 0.4862 | Test Acc: 84.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:51<00:00,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.5938 | Test Loss: 0.4946 | Test Acc: 84.30%\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel03(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units*2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel(input_shape=784, hidden_units=128, output_shape=len(class_dict))\n",
    "model_2.to(\"cpu\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;66;03m# only works when model was created with a class\u001b[39;00m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: acc}\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Calculate model 0 results on test dataset\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m model_0_results \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_fn\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m model_0_results\n",
      "Cell \u001b[1;32mIn[41], line 22\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(model, data_loader, loss_fn, accuracy_fn)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# Make predictions with the model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# Accumulate the loss and accuracy values per batch\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m, in \u001b[0;36mDoodleClassModel03.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test001\\ttt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "  2%|▏         | 1/50 [00:08<07:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.8905 | Test Loss: 0.6194 | Test Acc: 80.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:17<06:50,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 0.7554 | Test Loss: 0.5672 | Test Acc: 82.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:25<06:43,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 0.7025 | Test Loss: 0.5422 | Test Acc: 82.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:31<08:12, 10.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     58\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 59\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\adamw.py:187\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    177\u001b[0m         group,\n\u001b[0;32m    178\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m         state_steps,\n\u001b[0;32m    185\u001b[0m     )\n\u001b[1;32m--> 187\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\adamw.py:339\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 339\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\adamw.py:415\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    412\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    418\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel03(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units*2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel03(input_shape=784, hidden_units=256, output_shape=len(class_dict))  # Increased hidden units\n",
    "model_2.to(\"cpu\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=0.001, weight_decay=1e-5)  # Using AdamW optimizer\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)  # Reduce LR on plateau\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 50  # Increase the number of epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    scheduler.step(test_loss)  # Update the scheduler\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     60\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[81], line 24\u001b[0m, in \u001b[0;36mDoodleClassModel03.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel03(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units*2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel03(input_shape=784, hidden_units=512, output_shape=len(class_dict)) #hidden_units=256\n",
    "model_2.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    scheduler.step(test_loss)  # Update the scheduler\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0084, -0.0058,  0.0228,  ...,  0.0293,  0.0206, -0.0119],\n",
       "        [ 0.0009,  0.0123,  0.0233,  ..., -0.0127, -0.0286,  0.0204],\n",
       "        [-0.0308,  0.0149, -0.0223,  ...,  0.0130, -0.0236, -0.0194]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8812e-02, -3.5414e-02,  1.8694e-02,  8.5192e-03, -1.8164e-02,\n",
       "         3.4418e-02, -1.6152e-02,  2.4134e-02,  1.8119e-02, -2.9943e-02,\n",
       "         2.3034e-02, -3.2838e-02, -1.9786e-02, -5.9541e-03, -2.4073e-02,\n",
       "         3.4890e-02, -7.1636e-03,  1.4185e-02, -3.1890e-02,  2.0560e-02,\n",
       "        -1.1099e-02, -2.7167e-02,  5.2222e-03,  1.7302e-02,  3.0907e-02,\n",
       "        -2.1814e-02, -1.7575e-02,  6.8665e-03,  9.6879e-03,  1.3731e-02,\n",
       "         1.9603e-02, -8.0983e-03,  1.9842e-02,  2.6332e-02, -9.3293e-03,\n",
       "         2.5409e-02,  1.7449e-02,  3.1502e-02, -2.0295e-02, -1.7636e-02,\n",
       "        -1.0326e-02,  1.8117e-03,  2.1436e-02, -2.0388e-02,  1.7880e-02,\n",
       "        -1.2799e-02,  2.1575e-02, -1.6956e-03, -3.1289e-02, -1.9652e-02,\n",
       "        -2.5848e-02,  1.7713e-02, -2.3948e-02, -2.9757e-03,  7.7056e-03,\n",
       "        -1.9586e-02,  1.0303e-02, -3.4872e-02, -2.5554e-02, -3.2362e-02,\n",
       "        -1.0803e-02, -1.3011e-02,  5.1140e-03, -6.6041e-03,  1.6789e-02,\n",
       "         3.2743e-02,  1.2814e-02, -1.4070e-02, -3.3442e-02,  1.2936e-02,\n",
       "        -1.7695e-02,  1.8174e-02,  2.3874e-02,  1.3776e-02,  3.3511e-02,\n",
       "         3.3921e-02,  7.5621e-03, -2.6023e-02,  3.1909e-02, -1.6946e-02,\n",
       "        -1.6870e-02,  2.9885e-02,  2.7672e-02,  1.0791e-02,  2.2387e-03,\n",
       "        -3.0042e-02, -3.7073e-03,  3.4255e-02,  9.0950e-03,  3.0578e-03,\n",
       "        -7.4161e-03, -1.2457e-02,  2.1287e-02,  2.2024e-03,  2.3235e-02,\n",
       "        -6.3214e-03,  1.5604e-02,  1.4742e-02,  5.6951e-03,  2.2442e-02,\n",
       "         2.2380e-02,  3.3104e-02,  2.7456e-02, -9.1320e-03, -3.0236e-02,\n",
       "         6.5292e-03, -3.1172e-04, -9.3152e-03, -5.9805e-03,  1.6797e-03,\n",
       "         2.6058e-02,  1.1134e-02, -1.2682e-02, -1.4687e-02, -8.8440e-03,\n",
       "        -1.3804e-02,  3.2115e-02,  1.8916e-02,  3.2249e-02,  1.1406e-04,\n",
       "         7.2025e-03,  1.2384e-02, -3.3805e-02,  3.1892e-03, -2.3889e-03,\n",
       "        -2.0023e-02, -2.7712e-02,  3.1617e-02,  2.9038e-02,  1.6552e-02,\n",
       "         3.4080e-02, -1.4493e-02, -6.1687e-03,  1.3522e-02, -5.9009e-03,\n",
       "        -7.0084e-03, -2.9520e-02,  9.5950e-03, -2.1583e-02,  1.2988e-03,\n",
       "         3.4821e-02, -1.0993e-02, -1.1257e-02,  2.1547e-02, -1.3130e-02,\n",
       "        -3.0671e-03,  3.3348e-02, -1.4642e-02, -2.5550e-02, -1.9987e-02,\n",
       "        -9.9016e-03, -1.6946e-02, -1.8533e-02,  1.4427e-02,  6.0692e-03,\n",
       "        -1.1431e-02, -2.7747e-02, -1.1245e-02, -1.5073e-02, -1.1622e-02,\n",
       "        -3.2219e-02,  7.6939e-03, -2.6240e-02, -2.7814e-02, -2.9180e-02,\n",
       "         1.4907e-02, -2.1501e-02, -1.4741e-02,  2.7994e-02,  1.8963e-02,\n",
       "         2.0478e-02, -3.3911e-02, -2.5611e-02, -1.3483e-02,  2.9503e-02,\n",
       "         3.6536e-03, -2.6711e-02,  2.2364e-04, -2.7738e-02, -7.8250e-03,\n",
       "        -9.8205e-03,  3.0916e-02,  1.1062e-02, -6.2275e-03,  6.0332e-03,\n",
       "        -1.0309e-02,  1.4032e-02,  1.4127e-02,  9.5909e-03, -1.3921e-02,\n",
       "         3.0470e-02, -5.1561e-03, -1.3904e-02,  2.2368e-02,  2.9109e-02,\n",
       "         3.5541e-02,  1.0581e-02, -1.2175e-02,  1.8139e-02,  3.0641e-02,\n",
       "        -3.5030e-02, -4.4245e-03, -2.4356e-02,  6.6557e-03,  1.4771e-02,\n",
       "        -7.3781e-03, -2.9878e-03,  1.6075e-02, -6.0025e-03, -2.9992e-02,\n",
       "         2.8576e-02, -1.7976e-02, -3.9235e-03,  3.3688e-03, -2.1463e-03,\n",
       "        -3.3596e-02,  1.6386e-02, -1.6223e-02, -1.8524e-02,  8.5327e-03,\n",
       "        -1.8638e-02, -1.6506e-02, -1.2034e-02, -1.3417e-02, -1.4916e-02,\n",
       "        -9.6316e-03,  9.2813e-03, -2.8901e-02, -2.1617e-02,  5.2112e-04,\n",
       "         4.9667e-03,  1.9725e-02, -2.5087e-02,  1.1400e-02,  2.0300e-02,\n",
       "         1.9831e-02, -3.3265e-02, -1.3629e-02, -3.0699e-02, -2.2600e-02,\n",
       "         1.9892e-02, -5.3328e-03,  1.5168e-02, -2.0965e-02,  5.4271e-03,\n",
       "        -2.1602e-02,  1.7853e-02, -1.5621e-02, -8.9557e-03, -3.0987e-02,\n",
       "         1.1798e-04,  3.3910e-02,  1.7335e-02, -1.9055e-02,  4.8031e-04,\n",
       "        -3.9160e-03, -2.8753e-02,  2.8003e-02,  5.7574e-04,  7.5213e-03,\n",
       "        -1.4422e-02, -1.6711e-02,  5.8889e-03,  1.3204e-02,  8.0106e-03,\n",
       "        -1.7212e-02,  3.4675e-02, -5.2573e-03, -2.1872e-02, -1.6704e-02,\n",
       "         3.5158e-02,  1.7211e-06, -4.8480e-03, -1.4863e-02, -9.3623e-03,\n",
       "        -3.0080e-02, -2.8382e-02,  2.0903e-02,  3.0552e-02,  3.4082e-02,\n",
       "        -2.5784e-02,  1.9317e-02, -2.2106e-02,  2.1306e-02,  2.5770e-02,\n",
       "         2.7638e-02,  2.5716e-02,  2.2342e-02,  6.9513e-04,  1.6410e-02,\n",
       "        -1.2778e-02,  1.5551e-02, -1.1479e-02, -5.9969e-04, -3.1085e-02,\n",
       "        -9.3384e-03, -1.8781e-02, -1.2047e-02, -2.2807e-02, -3.2123e-02,\n",
       "         2.3256e-03,  2.3178e-02,  3.2527e-02,  2.0841e-02, -1.8512e-02,\n",
       "        -3.5322e-02,  1.3547e-02,  2.0011e-02, -3.0661e-02,  1.2807e-02,\n",
       "         3.0196e-02,  2.1634e-03, -2.1516e-02,  2.9281e-02,  1.5251e-02,\n",
       "         2.3650e-02, -2.4153e-02,  2.0784e-02, -2.4395e-02,  3.5339e-02,\n",
       "        -1.5132e-02,  2.1520e-02,  7.1515e-03,  9.4647e-03, -5.4770e-03,\n",
       "         1.4669e-02, -1.4885e-02, -3.3664e-02, -1.3722e-02,  2.7984e-02,\n",
       "        -9.4025e-03,  1.1226e-02, -1.3205e-02,  2.6791e-02,  2.1375e-02,\n",
       "         1.2608e-02, -1.8256e-02, -2.9183e-02,  1.3446e-03, -2.0952e-02,\n",
       "         2.9364e-02, -3.4321e-02,  1.5960e-02,  3.5604e-02,  1.7887e-02,\n",
       "         1.2178e-02, -3.4362e-02,  3.4353e-02, -6.1088e-03, -3.3372e-02,\n",
       "         3.5256e-02, -1.4533e-02, -2.5267e-03,  3.2689e-02, -2.4757e-02,\n",
       "        -2.5268e-02,  5.8078e-03, -4.7805e-03,  8.2265e-03, -2.9954e-02,\n",
       "         1.0681e-03, -1.5884e-02, -1.7557e-02, -3.2701e-02,  1.8936e-02,\n",
       "         6.8799e-03, -3.0194e-02,  2.8341e-02,  1.0772e-02,  6.6273e-03,\n",
       "        -2.0969e-02,  5.3859e-03,  3.4412e-02,  2.4496e-02, -2.7817e-02,\n",
       "         3.2601e-02,  2.7682e-03,  1.7177e-02,  2.7738e-02,  3.0447e-02,\n",
       "        -2.7841e-02,  3.1274e-02, -2.4256e-02,  2.6774e-03, -2.4956e-02,\n",
       "        -7.8298e-03, -1.6230e-03, -4.2727e-03, -5.6420e-03,  2.8168e-03,\n",
       "         3.5229e-02,  2.0750e-02,  1.9981e-02,  1.4295e-02,  2.7649e-02,\n",
       "        -1.6482e-03,  2.8397e-03,  7.3499e-03, -3.1148e-02, -2.8770e-02,\n",
       "         4.3786e-03, -1.3973e-02, -6.5535e-04, -8.1950e-03,  5.5594e-03,\n",
       "         2.3236e-02, -1.1844e-02,  2.8602e-02,  2.8199e-02, -2.7411e-02,\n",
       "        -2.7580e-02, -2.8896e-02, -1.9572e-02, -1.3903e-02, -2.6882e-03,\n",
       "        -8.6854e-03, -1.8045e-02, -1.1346e-02, -1.2919e-02,  3.5037e-02,\n",
       "        -1.3237e-02, -2.5571e-02,  1.4846e-02, -2.0629e-03,  2.7342e-02,\n",
       "         2.2315e-02,  3.2812e-02, -2.6156e-02,  2.2959e-02,  2.9973e-02,\n",
       "        -1.7637e-02,  3.2828e-02,  2.6773e-02,  3.9111e-04,  1.7220e-02,\n",
       "        -1.2486e-02, -3.1149e-02,  9.0279e-03,  1.0647e-02, -2.3341e-02,\n",
       "         1.7321e-02, -3.0508e-02,  3.0737e-02,  3.4585e-02,  9.7235e-03,\n",
       "        -2.2408e-02,  1.7381e-02,  6.0863e-03,  9.7114e-03,  1.1738e-02,\n",
       "         2.7191e-02, -1.5351e-02, -8.0338e-03,  9.7395e-03,  3.8915e-03,\n",
       "         2.8799e-02, -1.8759e-02, -1.3006e-03,  6.6733e-03, -9.4853e-03,\n",
       "         2.4347e-02,  3.9065e-03, -3.3008e-02, -3.8691e-03, -1.6198e-02,\n",
       "         3.4704e-03, -4.1506e-03, -3.5426e-02, -6.5080e-03, -3.4207e-03,\n",
       "        -1.0531e-02,  3.2814e-02, -7.7928e-03,  2.2943e-02,  8.8471e-03,\n",
       "        -3.0148e-02,  8.3924e-03,  2.9598e-02, -2.3361e-02, -2.3085e-02,\n",
       "         3.4959e-02,  2.8697e-02, -1.9925e-02,  2.1491e-02, -4.5690e-03,\n",
       "        -3.5212e-02,  2.6833e-03,  1.1539e-02, -1.0713e-02,  1.2424e-02,\n",
       "        -3.0539e-02,  2.4751e-02,  3.0448e-02,  1.9695e-02,  6.0533e-03,\n",
       "         1.1762e-02, -2.5841e-02, -8.9241e-03, -3.4096e-03, -1.9875e-02,\n",
       "        -2.6379e-02,  2.4022e-02,  2.4237e-02, -3.2435e-02,  1.1365e-02,\n",
       "         1.4529e-02,  3.3927e-02,  2.0663e-02,  3.2833e-02, -1.1692e-02,\n",
       "         2.5014e-02,  2.9050e-02, -3.3726e-02, -2.8668e-02,  7.2288e-03,\n",
       "         1.9501e-02, -1.7743e-02,  3.9334e-03, -5.1849e-05,  9.0396e-03,\n",
       "        -1.9191e-02,  2.0144e-02,  2.3750e-02, -2.4781e-02,  3.4598e-04,\n",
       "         9.9093e-05,  1.1468e-02,  3.3271e-02,  1.0474e-02, -8.1151e-03,\n",
       "         3.2076e-02, -1.3590e-02, -1.0370e-02,  2.4384e-03, -2.7197e-02,\n",
       "         3.8863e-03, -2.4231e-02,  3.6722e-03,  3.4217e-03,  4.9436e-03,\n",
       "        -3.0118e-02, -3.3921e-02,  1.6433e-02,  3.0628e-02, -3.1692e-02,\n",
       "         1.3228e-02, -2.6290e-02,  2.2390e-03,  3.3227e-02,  2.7094e-02,\n",
       "        -2.6112e-02,  2.2092e-02,  1.8654e-02, -5.4780e-05,  3.4597e-02,\n",
       "        -1.4185e-02, -8.3113e-03, -1.8051e-02,  5.1392e-03,  9.4567e-03,\n",
       "         2.8457e-03, -2.0094e-02, -1.6077e-02,  1.2148e-02, -3.1315e-02,\n",
       "        -5.5619e-03, -7.9113e-04, -1.4470e-02,  1.2997e-02,  2.3019e-03,\n",
       "         2.5074e-02, -3.1718e-03, -2.2541e-02,  2.5234e-02,  4.9875e-03,\n",
       "        -1.0229e-02, -2.3539e-02,  8.3710e-03, -3.4063e-02,  9.4064e-03,\n",
       "         1.7659e-02, -3.1329e-02, -2.3381e-02, -2.5676e-02,  3.0018e-02,\n",
       "         3.0361e-02,  2.2357e-03,  3.3529e-02, -1.1939e-02, -3.4672e-02,\n",
       "         1.2475e-02,  1.2652e-02,  3.5014e-02, -1.0338e-05,  2.7371e-02,\n",
       "         2.1892e-02,  2.5849e-02, -3.5440e-02,  2.9409e-02, -3.0445e-02,\n",
       "         3.2018e-02, -1.4800e-03, -3.1651e-02,  9.9319e-03, -7.2695e-03,\n",
       "        -3.2078e-02, -1.2948e-02,  3.6823e-03,  2.7816e-02, -1.0980e-02,\n",
       "         2.1213e-02,  4.4724e-03,  1.2039e-03,  6.8338e-03, -2.3438e-02,\n",
       "        -2.9686e-02, -1.1222e-02, -3.3281e-02, -1.6655e-02,  1.6431e-03,\n",
       "        -2.3522e-02, -1.4814e-02, -6.4174e-03, -4.1144e-03, -1.9715e-02,\n",
       "         1.7161e-03, -5.0129e-03,  1.0214e-02,  1.6131e-03, -1.7835e-02,\n",
       "        -1.2296e-02, -4.1163e-04, -2.2285e-02,  2.7959e-03, -2.5216e-03,\n",
       "         1.3856e-02,  8.4523e-03,  2.6728e-02, -1.1773e-03, -2.6389e-02,\n",
       "         1.5965e-02, -2.4137e-02, -4.9090e-03, -1.1564e-02, -1.3552e-02,\n",
       "        -1.3214e-02,  3.2819e-02,  2.0445e-02,  9.8978e-03,  3.3534e-02,\n",
       "        -3.2365e-02,  5.7631e-03,  2.2863e-02,  2.3725e-02, -2.8423e-02,\n",
       "         1.2710e-02, -2.0346e-02, -1.3955e-03, -7.4525e-03, -2.9820e-02,\n",
       "        -3.4068e-02, -2.3496e-02,  1.6206e-02,  1.6337e-02, -3.1063e-02,\n",
       "        -1.3412e-02,  5.8337e-04, -3.0878e-02,  2.0123e-02,  3.2348e-02,\n",
       "         6.3165e-03, -2.6536e-02,  8.3281e-03,  1.2965e-02,  2.8280e-02,\n",
       "        -1.9002e-02, -3.1997e-02,  1.5691e-02,  2.2234e-03, -1.1637e-02,\n",
       "        -1.4965e-02, -1.8415e-02,  2.8911e-02, -6.1621e-03,  2.5757e-02,\n",
       "         3.1880e-02,  4.5241e-03,  7.6078e-03, -2.4323e-03,  2.4529e-02,\n",
       "        -3.2182e-02,  1.4203e-02, -5.8189e-03,  1.6308e-02, -1.5907e-04,\n",
       "        -1.3224e-02, -3.8554e-03,  1.9285e-02, -2.4805e-02, -2.2575e-02,\n",
       "        -7.9999e-03, -1.2954e-02,  1.4030e-02,  2.4035e-02, -3.8249e-03,\n",
       "         3.3770e-02, -1.4922e-02, -2.0041e-02, -1.5989e-02, -2.4606e-02,\n",
       "        -2.5033e-02, -3.1822e-02, -3.1653e-02,  9.3214e-03,  2.1781e-02,\n",
       "         3.3530e-02,  3.0421e-02, -2.0569e-03,  6.9302e-03, -2.1369e-02,\n",
       "         1.3804e-02, -3.4244e-02,  1.6845e-02,  6.5506e-03,  1.0975e-02,\n",
       "         1.3077e-02, -1.6397e-02,  3.0647e-02,  1.7045e-02,  2.3461e-02,\n",
       "         4.7171e-03,  2.2428e-02,  2.4036e-02,  1.6187e-02,  2.7206e-02,\n",
       "        -6.4778e-03,  2.3483e-02,  8.2285e-03,  2.1489e-02,  3.4786e-02,\n",
       "         8.0870e-03, -1.3256e-02, -3.5534e-03,  1.0055e-02,  6.0218e-03,\n",
       "        -9.7561e-03,  2.7983e-02,  2.1457e-02, -2.2184e-02, -2.9101e-03,\n",
       "        -3.4283e-03, -2.2383e-02,  5.2091e-03, -1.0964e-02, -1.8438e-02,\n",
       "        -1.1484e-02, -1.2734e-02,  3.4166e-02,  1.3699e-02,  3.5147e-02,\n",
       "         9.7927e-03,  8.0868e-03,  1.4682e-02,  6.1297e-03, -2.8028e-02,\n",
       "        -2.1120e-02, -8.6528e-03,  5.3690e-04,  1.3454e-02,  1.3752e-02,\n",
       "         2.8333e-02,  7.2705e-03, -1.2189e-02, -2.6858e-02, -2.6158e-02,\n",
       "        -2.1333e-02,  1.0772e-02,  1.9295e-02,  3.1247e-02, -1.1297e-02,\n",
       "        -2.8172e-03, -1.0606e-02, -1.3050e-04,  1.1464e-02], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(model_2.parameters())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_2.state_dict(), 'model002_50k_1st_512_hid.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DoodleClassModel03',\n",
       " 'model_loss': 0.35276123881340027,\n",
       " 'model_acc': 89.997}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Move data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Move model to the device\n",
    "model_2.to(device)\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'model_name': 'DoodleClassModel03',\n",
    " 'model_loss': 0.3829771876335144,\n",
    " 'model_acc': 88.561}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119364, 784)\n",
      "Saved 10000 images to C:/test001/ttt/data/book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy')\n",
    "\n",
    "# Check the shape of the data\n",
    "print(np.shape(array_data))  # Should output (126527, 784)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = 'C:/test001/ttt/data/book'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the number of images to save\n",
    "num_images_to_save = 10000\n",
    "\n",
    "# Save images\n",
    "for idx in range(min(num_images_to_save, len(array_data))):\n",
    "    img = array_data[idx]\n",
    "    \n",
    "    # Reshape the image to 28x28 pixels\n",
    "    image_reshaped = img.reshape(28, 28)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_reshaped, cmap='gray')\n",
    "    plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "    \n",
    "    # Save the image\n",
    "    output_path = os.path.join(output_dir, f'image_{idx}.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Clear the plot to avoid overlap\n",
    "    plt.clf()\n",
    "\n",
    "print(f\"Saved {min(num_images_to_save, len(array_data))} images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
