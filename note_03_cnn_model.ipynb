{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (400000, 1, 28, 28)\n",
      "X_test shape: (100000, 1, 28, 28)\n",
      "y_train shape: (400000,)\n",
      "y_test shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=50000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 1,28, 28)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: torch.Size([400000, 1, 28, 28])\n",
      "X_test shape after reshaping: torch.Size([100000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (100000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (400000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32) #dtype=torch.float32\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[5]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x00000287FF932A50>, <torch.utils.data.dataloader.DataLoader object at 0x0000028820864470>)\n",
      "Length of train dataloader: 12500 batches of 32\n",
      "Length of test dataloader: 3125 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 2, 4, 9, 0, 9, 3, 4, 1, 7, 5, 1, 8, 2, 3, 8, 2, 2, 9, 1, 9, 3, 9, 0,\n",
       "        0, 9, 2, 8, 3, 1, 4, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test001\\ttt\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "  3%|▎         | 1/30 [00:27<13:06, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.3991 | Test Loss: 0.2465 | Test Acc: 91.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:53<12:31, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.3024 | Test Loss: 0.2112 | Test Acc: 93.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:20<11:58, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.2786 | Test Loss: 0.2038 | Test Acc: 93.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:46<11:28, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.2655 | Test Loss: 0.1975 | Test Acc: 93.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:19<11:59, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.2582 | Test Loss: 0.1932 | Test Acc: 93.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:53<12:17, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.2522 | Test Loss: 0.1904 | Test Acc: 93.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [03:32<12:47, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.2477 | Test Loss: 0.1853 | Test Acc: 94.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [04:15<13:21, 36.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.2439 | Test Loss: 0.1882 | Test Acc: 93.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [04:49<12:30, 35.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.2405 | Test Loss: 0.1847 | Test Acc: 94.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [05:26<12:00, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.2382 | Test Loss: 0.1836 | Test Acc: 94.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [05:58<11:00, 34.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.2368 | Test Loss: 0.1829 | Test Acc: 94.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [06:37<10:52, 36.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.2344 | Test Loss: 0.1799 | Test Acc: 94.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [07:04<09:25, 33.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.2320 | Test Loss: 0.1813 | Test Acc: 94.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [07:30<08:19, 31.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.2298 | Test Loss: 0.1788 | Test Acc: 94.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [08:06<08:06, 32.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.2295 | Test Loss: 0.1790 | Test Acc: 94.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [08:48<08:17, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.2278 | Test Loss: 0.1784 | Test Acc: 94.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [09:31<08:11, 37.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.2281 | Test Loss: 0.1758 | Test Acc: 94.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [10:14<07:51, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.2266 | Test Loss: 0.1747 | Test Acc: 94.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [10:58<07:25, 40.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.2235 | Test Loss: 0.1767 | Test Acc: 94.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [11:40<06:51, 41.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.2237 | Test Loss: 0.1749 | Test Acc: 94.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [12:23<06:14, 41.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.2232 | Test Loss: 0.1739 | Test Acc: 94.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [13:06<05:36, 42.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.2225 | Test Loss: 0.1763 | Test Acc: 94.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [13:49<04:55, 42.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.2222 | Test Loss: 0.1723 | Test Acc: 94.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [14:25<04:03, 40.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.2217 | Test Loss: 0.1723 | Test Acc: 94.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [14:51<03:00, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.2208 | Test Loss: 0.1733 | Test Acc: 94.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [15:23<02:19, 34.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.2184 | Test Loss: 0.1736 | Test Acc: 94.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [16:06<01:51, 37.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.2200 | Test Loss: 0.1711 | Test Acc: 94.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [16:37<01:11, 35.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.2176 | Test Loss: 0.1732 | Test Acc: 94.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [17:03<00:32, 32.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.2183 | Test Loss: 0.1729 | Test Acc: 94.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [17:29<00:00, 35.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.2169 | Test Loss: 0.1708 | Test Acc: 94.45%\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel04(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128*3*3, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features=256, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel04(input_channels=1, output_shape=len(class_dict)) # For grayscale images\n",
    "model_2.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    scheduler.step(test_loss)  # Update the scheduler\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DoodleClassModel04',\n",
       " 'model_loss': 0.1708349734544754,\n",
       " 'model_acc': 94.455}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Move data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Move model to the device\n",
    "model_2.to(device)\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKpElEQVR4nO3cT4iVdRvG8d9xNMmxMsLAEaJCQoTQiCJF0IQWbYWgXURUFCFIUrRIa1NQROAmciMkZeEmN22Kwk0hidNCrMQIs6Bs1P4QCTWed/HSBW+9i3M/OWdmjp/P+lw8h8jz9Vl49/r9fr8BQGttwWx/AQDmDlEAIEQBgBAFAEIUAAhRACBEAYAQBQBi4aAf7PV6M/k9AJhhg/xbZW8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCADHwQTwYdbfffnt50+VQ5JEjR8obGBZvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDR6/f7/YE+2OHwF8yWVatWlTeTk5PlzdKlS8ubHTt2lDettfbKK6902sFfBvm596YAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQCyc7S/A7FuzZk15Mz09Xd58+eWX5U1r3S70vvnmm+XN+Ph4edPFuXPnhvIc6MKbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAED0+v1+f6APdjhKxvxw8ODB8ubee+8tb15++eXyprXWDhw4UN5MTk6WN8eOHStvuhwTXLlyZXnTWmvff/99px38ZZCfe28KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAOEgHm3ZsmXlzQsvvFDePProo+VNa62dPn26vLnhhhvKm6+//rq8OXv2bHlz5513ljdwKTiIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgCxcLa/ALPvp59+Km8ef/zx8ubjjz8ub1prbd++feXNoUOHyptNmzaVN6+++mp5A3OZNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA6PX7/f5AH+z1Zvq7MOLGxsY67U6ePFneXHXVVeXNn3/+Wd5s3LixvNm5c2d501pra9as6bSr+vnnn8ubd955p7zZv39/edNaa7/++munHa0N8nPvTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCWVTq655pryZsGCbn8Hee+998qbu+66q7x5//33y5v169eXNwP+kfuHw4cPd9pVLV++vLxZu3ZtefPjjz+WN621tmXLlvLm2LFjnZ41alxJBaBEFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwEG8IVq9eXd48+OCDnZ61YcOG8uaOO+4obxYvXlzejKIDBw6UN9u3b+/0rO+++67TbhjWrVtX3rz77rudnrVo0aLypsufi1OnTpU3c52DeACUiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQl/VBvLGxsfLmpZdeKm+2bdtW3ly4cKG8aa21Tz/9tLz55JNPypszZ86UN7///nt501pr09PT5c3U1FR588svv5Q3H330UXnDf61atarTrsv/4x988EF5c99995U3c52DeACUiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQI3MQ74orrihv3nrrrfJm69at5c3u3bvLm+eff768aa218+fPd9rBfPH000+XNy+++GJ5s379+vLm8OHD5c0wOYgHQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMTIXEl99tlny5vnnnuuvHnsscfKmz179pQ3wP935ZVXljcnTpwob7766qvyZvPmzeXNMLmSCkCJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxMgfxjh49Wt6cPXu2vLnnnnvKG2B2PfLII+XN66+/Xt7cdttt5U1rrX322WeddlUO4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBALJztL/B3K1eu7LRbt25debN9+/ZOzwLml+PHjw/lOUuXLh3Kc2aSNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAmHMH8W666aZOu16vV95MTk52ehbAqPKmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBz7iDeMF28eHG2vwIwBIsWLRrKc0bhN8WbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgBxWV9JBS4PmzdvLm+6XDw9ceJEeTPXeFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACAfxgHnl2muvLW+2bdtW3rz99tvlzdTUVHkz13hTACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAIg5dxDvwoULQ3vWkiVLhvYs4NJ45plnypvx8fHyZteuXeXNKPCmAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBz7iDeDz/8MLRnrVixYmjPAv7XxMREp90TTzxR3uzdu7e8OXnyZHkzCrwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAESv3+/3B/pgrzfT36W11trY2Fin3bffflveHDp0qLy5//77yxvgn1577bVOuwceeKC8ueWWW8qbLr8pc90gP/feFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIhbP9Bf5uenq6027v3r3lzZNPPlneXH/99eXNmTNnyhuYT1avXl3ePPTQQ52etXv37vJmFC+ezhRvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDR6/f7/YE+2OvN9Hf5V2688cby5osvvihvPv/88/Lm4YcfLm+OHDlS3sClcPPNN5c3H374YXmzePHi8qa11m699dbyZmpqqtOzRs0gP/feFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBiZA7idXH33XeXN2+88UZ5MzExMZTntNbarl27yptvvvmmvFmwoP73ieuuu668aa215cuXlzebNm0qbzZs2FDedD3qNixXX311edPlz8X58+fLmy1btpQ3rbV2/PjxTjscxAOgSBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAuKwP4nUxPj5e3jz11FPlzY4dO8qb1lpbsmRJpx2tnT59urz5448/ZuCbXDq//fZbeXPw4MHyZs+ePeVNl//e/DsO4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA4UrqHDUxMdFpt3Xr1vKmy+XXLs6dO9dpNzU1Vd4cPXq0vDl16lR5A/OJK6kAlIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEA7iAVwmHMQDoEQUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgFg46AcHvJsHwDzmTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYD4D81utygJe6BfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[9].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    \n",
    "    # Reshape the image to (1, 1, 28, 28) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    \n",
    "    # Check if CUDA (GPU) is available and move tensor to CUDA device\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_bat\n",
      "Predicted class index: 6\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(image_array)#(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_2.state_dict(), './models/cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoodleClassModel04(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.4, inplace=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout(p=0.4, inplace=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): Dropout(p=0.4, inplace=False)\n",
       "    (20): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test001\\ttt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoodleClassModel04(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.4, inplace=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout(p=0.4, inplace=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): Dropout(p=0.4, inplace=False)\n",
       "    (20): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel04(nn.Module):\n",
    "    def __init__(self, input_channels: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128*3*3, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features=256, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "    \n",
    "# Define the input channels and output shape\n",
    "input_channels = 1  # Assuming grayscale images\n",
    "output_shape = 10  # Assuming 10 classes for classification\n",
    "\n",
    "# Initialize the model\n",
    "model = DoodleClassModel04(input_channels=1, output_shape=len(class_dict))\n",
    "\n",
    "# Check if CUDA is available and move the model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('C:/test001/ttt/cnn_classifier/models/cnn_model.pth', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# # Example: Load an image and make a prediction\n",
    "# from PIL import Image\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# # Load the image\n",
    "# image = Image.open('C:/test001/ttt/bat.jpg').convert('L')  # Convert to grayscale\n",
    "\n",
    "# # Resize the image to 28x28\n",
    "# image = image.resize((28, 28))\n",
    "\n",
    "# # Convert the image to a tensor\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ])\n",
    "# image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('C:/test001/ttt/cnn_classifier/iamges/apple.jpg')\n",
    "\n",
    "# Resize the image to 28x28\n",
    "resized_image = image.resize((28, 28))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image_array = np.array(resized_image)\n",
    "\n",
    "# Ensure the image has the shape (28, 28, 1)\n",
    "# For grayscale, add a new axis to make it (28, 28, 1)\n",
    "image_array = np.expand_dims(image_array, axis=-1)\n",
    "\n",
    "print(np.shape(image_array))\n",
    "# Display the image\n",
    "plt.imshow(image_array.squeeze(), cmap='gray')\n",
    "plt.axis('off')  # Optional: to hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(678, 834)\n",
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('C:/test001/ttt/cnn_classifier/iamges/air.jpg').convert('L')  # Convert to grayscale\n",
    "\n",
    "# Invert the colors\n",
    "image = ImageOps.invert(image)\n",
    "print(np.shape(image))\n",
    "# Resize the image to 28x28\n",
    "resized_image = image.resize((28, 28))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image_array = np.array(resized_image)\n",
    "\n",
    "# Ensure the image has the shape (28, 28, 1)\n",
    "image_array = np.expand_dims(image_array, axis=-1)\n",
    "\n",
    "print(np.shape(image_array))  # Should output (28, 28, 1)\n",
    "plt.imshow(image_array.squeeze(), cmap='gray')\n",
    "plt.axis('off')  # Optional: to hide the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_airplane\n",
      "Predicted class index: 8\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(image_array)#(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(input_tensor))\n",
    "\n",
    "print(np.shape(image_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[66].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Ratio: 139:113\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "width = 834\n",
    "height = 678\n",
    "\n",
    "gcd = math.gcd(width, height)\n",
    "print(gcd)\n",
    "ratio_width = width // gcd\n",
    "ratio_height = height // gcd\n",
    "\n",
    "print(f\"Ratio: {ratio_width}:{ratio_height}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_with_center_crop(image_path, target_size):\n",
    "    image = Image.open(image_path)\n",
    "    old_size = image.size\n",
    "    ratio = max(old_size) / float(target_size)\n",
    "    new_size = tuple([int(x / ratio) for x in old_size])\n",
    "    image = image.resize(new_size, Image.LANCZOS)\n",
    "    left = (new_size[0] - target_size) / 2\n",
    "    top = (new_size[1] - target_size) / 2\n",
    "    right = (new_size[0] + target_size) / 2\n",
    "    bottom = (new_size[1] + target_size) / 2\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'C:/test001/ttt/cnn_classifier/iamges/air.jpg'\n",
    "target_size = 28  # Specify the target size as a single integer for square cropping\n",
    "cropped_image = resize_with_center_crop(image_path, target_size)\n",
    "\n",
    "# Display the cropped image (optional)\n",
    "cropped_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "def resize_with_padding1(image_path, target_size):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = ImageOps.invert(image)\n",
    "    old_size = image.size\n",
    "    ratio = float(target_size) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    image = image.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    new_image = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
    "    new_image.paste(image, ((target_size - new_size[0]) // 2, (target_size - new_size[1]) // 2))\n",
    "    return new_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'C:/test001/ttt/cnn_classifier/iamges/air.jpg'\n",
    "target_size = 28  # Specify the target size as a single integer for square cropping\n",
    "padded_image = resize_with_padding1(image_path, target_size)\n",
    "\n",
    "np.shape(padded_image)\n",
    "# Display the padded image (optional)\n",
    "padded_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "def resize_with_padding(image_path, target_size):\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = ImageOps.invert(image)  # Invert colors if needed\n",
    "    old_size = image.size\n",
    "    ratio = float(target_size) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    image = image.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    new_image = Image.new(\"L\", (target_size, target_size), 0)  # Create new image with black padding\n",
    "    new_image.paste(image, ((target_size - new_size[0]) // 2, (target_size - new_size[1]) // 2))\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "# Example usage\n",
    "image_path = 'C:/test001/ttt/cnn_classifier/iamges/air.jpg'\n",
    "target_size = 28  # Specify the target size as a single integer for square cropping\n",
    "padded_image = resize_with_padding(image_path, target_size)\n",
    "\n",
    "# Convert to NumPy array and ensure shape is (28, 28, 1)\n",
    "image_array1 = np.array(padded_image)\n",
    "image_array1 = np.expand_dims(image_array1, axis=-1)\n",
    "\n",
    "print(np.shape(image_array1))  # Should output (28, 28, 1)\n",
    "padded_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(678, 834)\n",
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.open('C:/test001/ttt/cnn_classifier/iamges/air.jpg').convert('L')  # Convert to grayscale\n",
    "\n",
    "# Invert the colors\n",
    "image = ImageOps.invert(image)\n",
    "print(np.shape(image))\n",
    "# Resize the image to 28x28\n",
    "resized_image = image.resize((28, 28))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image_array = np.array(resized_image)\n",
    "\n",
    "# Ensure the image has the shape (28, 28, 1)\n",
    "image_array = np.expand_dims(image_array, axis=-1)\n",
    "\n",
    "print(np.shape(image_array))  # Should output (28, 28, 1)\n",
    "plt.imshow(image_array.squeeze(), cmap='gray')\n",
    "plt.axis('off')  # Optional: to hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_airplane\n",
      "Predicted class index: 8\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(image_array)#(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(padded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Example: Define your PyTorch model\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model.eval()\n",
    "\n",
    "# Example: Export PyTorch model to ONNX\n",
    "input_shape = (32, 1, 28, 28)  # Example input shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(input_shape, device='cuda')\n",
    "output_path = 'model.onnx'\n",
    "torch.onnx.export(model, dummy_input, output_path, verbose=True, input_names=['input'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your model is already loaded and set to evaluation mode\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device=device)  # Adjust the input size as needed\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\", input_names=[\"image\"], output_names=[\"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.5.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Fail to import BlobReader from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n",
      "Fail to import BlobWriter from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coremltools.converters.nnssa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01monnx_coreml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/test001/ttt/cnn_classifier/model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\onnx_coreml\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unicode_literals\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# onnx-coreml version\u001b[39;00m\n\u001b[0;32m      9\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\onnx_coreml\\converter.py:35\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvAddFuser, DropoutRemover, \\\n\u001b[0;32m     28\u001b[0m     ReshapeInitTensorFuser, BNBroadcastedMulFuser, BNBroadcastedAddFuser, \\\n\u001b[0;32m     29\u001b[0m     PixelShuffleFuser, OutputRenamer, AddModelInputsOutputs, \\\n\u001b[0;32m     30\u001b[0m     ConstantsToInitializers, ImageScalerRemover, ShapeOpRemover, ConstantRemover, \\\n\u001b[0;32m     31\u001b[0m     ConstantFillToInitializers, ReshapeTransposeReshape_pattern1, CastOpRemover, \\\n\u001b[0;32m     32\u001b[0m     DeadCodeElimination\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# ML model passes\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcoremltools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnnssa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoreml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_pass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlmodel_passes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_disconnected_layers, transform_conv_crop\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_error_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ErrorHandling\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_graph \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'coremltools.converters.nnssa'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_coreml import convert\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"C:/test001/ttt/cnn_classifier/model.onnx\")\n",
    "\n",
    "# Convert the ONNX model to Core ML\n",
    "coreml_model = convert(\n",
    "    model=onnx_model,\n",
    "    image_input_names=[\"image\"],\n",
    "    image_output_names=[\"category\"],\n",
    "    minimum_ios_deployment_target='13'\n",
    ")\n",
    "\n",
    "# Save the Core ML model\n",
    "coreml_model.save(\"model.mlmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "# Define the new model architecture\n",
    "input_features = ct.models.datatypes.Array(28, 28)\n",
    "output_features = ct.models.datatypes.Dictionary(ct.models.datatypes.String)\n",
    "\n",
    "builder = ct.models.neural_network.NeuralNetworkBuilder(\n",
    "    input_features=[('image', input_features)],\n",
    "    output_features=[('category softmax scores', output_features), ('category', ct.models.datatypes.String)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers to the model\n",
    "builder.add_convolution(\n",
    "    name='conv1', \n",
    "    kernel_channels=1, \n",
    "    output_channels=32, \n",
    "    height=3, \n",
    "    width=3, \n",
    "    stride_height=1, \n",
    "    stride_width=1, \n",
    "    border_mode='valid', \n",
    "    groups=1, \n",
    "    W=np.random.rand(32, 1, 3, 3),  # Random weights for example\n",
    "    b=np.random.rand(32),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='image', \n",
    "    output_name='conv1_output'\n",
    ")\n",
    "builder.add_activation(name='relu1', non_linearity='RELU', input_name='conv1_output', output_name='relu1_output')\n",
    "builder.add_pooling(name='pool1', height=2, width=2, stride_height=2, stride_width=2, layer_type='MAX', input_name='relu1_output', output_name='pool1_output' , padding_type='VALID')\n",
    "\n",
    "# Save the model\n",
    "model = ct.models.MLModel(builder.spec)\n",
    "model.save('new_model.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dictionary(String)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "\n",
    "# Define the new model architecture\n",
    "input_features = ct.models.datatypes.Array(28, 28)\n",
    "output_features = ct.models.datatypes.Dictionary(ct.models.datatypes.String)\n",
    "\n",
    "builder = ct.models.neural_network.NeuralNetworkBuilder(\n",
    "    input_features=[('image', input_features)],\n",
    "    output_features=[('category softmax scores', output_features), ('category', ct.models.datatypes.String)])\n",
    "\n",
    "# Add layers to the model\n",
    "builder.add_convolution(\n",
    "    name='conv1', \n",
    "    kernel_channels=1, \n",
    "    output_channels=32, \n",
    "    height=3, \n",
    "    width=3, \n",
    "    stride_height=1, \n",
    "    stride_width=1, \n",
    "    border_mode='valid', \n",
    "    groups=1, \n",
    "    W=np.random.rand(32, 1, 3, 3),  # Random weights for example\n",
    "    b=np.random.rand(32),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='image', \n",
    "    output_name='conv1_output'\n",
    ")\n",
    "builder.add_activation(name='relu1', non_linearity='RELU', input_name='conv1_output', output_name='relu1_output')\n",
    "builder.add_batchnorm(\n",
    "    name='batchnorm1', \n",
    "    channels=32, \n",
    "    gamma=np.random.rand(32), \n",
    "    beta=np.random.rand(32), \n",
    "    mean=np.random.rand(32), \n",
    "    variance=np.random.rand(32), \n",
    "    input_name='relu1_output', \n",
    "    output_name='batchnorm1_output'\n",
    ")\n",
    "builder.add_pooling(name='pool1', height=2, width=2, stride_height=2, stride_width=2, layer_type='MAX', input_name='batchnorm1_output', output_name='pool1_output' , padding_type='VALID')\n",
    "\n",
    "builder.add_convolution(\n",
    "    name='conv2', \n",
    "    kernel_channels=32, \n",
    "    output_channels=64, \n",
    "    height=3, \n",
    "    width=3, \n",
    "    stride_height=1, \n",
    "    stride_width=1, \n",
    "    border_mode='valid', \n",
    "    groups=1, \n",
    "    W=np.random.rand(64, 32, 3, 3),  # Random weights for example\n",
    "    b=np.random.rand(64),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='dropout1_output', \n",
    "    output_name='conv2_output'\n",
    ")\n",
    "builder.add_activation(name='relu2', non_linearity='RELU', input_name='conv2_output', output_name='relu2_output')\n",
    "builder.add_batchnorm(\n",
    "    name='batchnorm2', \n",
    "    channels=64, \n",
    "    gamma=np.random.rand(64), \n",
    "    beta=np.random.rand(64), \n",
    "    mean=np.random.rand(64), \n",
    "    variance=np.random.rand(64), \n",
    "    input_name='relu2_output', \n",
    "    output_name='batchnorm2_output'\n",
    ")\n",
    "builder.add_pooling(name='pool2', height=2, width=2, stride_height=2, stride_width=2, layer_type='MAX', input_name='batchnorm2_output', output_name='pool2_output' , padding_type='VALID')\n",
    "\n",
    "builder.add_convolution(\n",
    "    name='conv3', \n",
    "    kernel_channels=64, \n",
    "    output_channels=128, \n",
    "    height=3, \n",
    "    width=3, \n",
    "    stride_height=1, \n",
    "    stride_width=1, \n",
    "    border_mode='valid', \n",
    "    groups=1, \n",
    "    W=np.random.rand(128, 64, 3, 3),  # Random weights for example\n",
    "    b=np.random.rand(128),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='dropout2_output', \n",
    "    output_name='conv3_output'\n",
    ")\n",
    "builder.add_activation(name='relu3', non_linearity='RELU', input_name='conv3_output', output_name='relu3_output')\n",
    "builder.add_batchnorm(\n",
    "    name='batchnorm3', \n",
    "    channels=128, \n",
    "    gamma=np.random.rand(128), \n",
    "    beta=np.random.rand(128), \n",
    "    mean=np.random.rand(128), \n",
    "    variance=np.random.rand(128), \n",
    "    input_name='relu3_output', \n",
    "    output_name='batchnorm3_output'\n",
    ")\n",
    "builder.add_pooling(name='pool3', height=2, width=2, stride_height=2, stride_width=2, layer_type='MAX', input_name='batchnorm3_output', output_name='pool3_output' , padding_type='VALID')\n",
    "builder.add_flatten(name='flatten', input_name='dropout3_output', output_name='flatten_output',mode=1)\n",
    "\n",
    "builder.add_inner_product(\n",
    "    name='fc1', \n",
    "    input_channels=128*3*3, \n",
    "    output_channels=256, \n",
    "    W=np.random.rand(256, 128*3*3),  # Random weights for example\n",
    "    b=np.random.rand(256),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='flatten_output', \n",
    "    output_name='fc1_output'\n",
    ")\n",
    "builder.add_activation(name='relu4', non_linearity='RELU', input_name='fc1_output', output_name='relu4_output')\n",
    "builder.add_batchnorm(\n",
    "    name='batchnorm4', \n",
    "    channels=256, \n",
    "    gamma=np.random.rand(256), \n",
    "    beta=np.random.rand(256), \n",
    "    mean=np.random.rand(256), \n",
    "    variance=np.random.rand(256), \n",
    "    input_name='relu4_output', \n",
    "    output_name='batchnorm4_output'\n",
    ")\n",
    "\n",
    "builder.add_inner_product(\n",
    "    name='fc2', \n",
    "    input_channels=256, \n",
    "    output_channels=len(class_dict), \n",
    "    W=np.random.rand(len(class_dict), 256),  # Random weights for example\n",
    "    b=np.random.rand(len(class_dict)),  # Random biases for example\n",
    "    has_bias=True, \n",
    "    input_name='dropout4_output', \n",
    "    output_name='fc2_output'\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model = ct.models.MLModel(builder.spec)\n",
    "model.save('new_model02.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'coremltools' has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ... (define the model architecture as before)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a Core ML model from the neural network builder\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'coremltools' has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "\n",
    "# ... (define the model architecture as before)\n",
    "\n",
    "# Create a Core ML model from the neural network builder\n",
    "# model = ct.models.MLModel(builder.spec)\n",
    "\n",
    "# # Define the training and testing data loaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = ct.loss.CrossEntropyLoss()\n",
    "optimizer = ct.optimizer.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):  # train for 30 epochs\n",
    "    for batch in train_dataloader:\n",
    "        # Get the input and target tensors\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(ct.device)\n",
    "        targets = targets.to(ct.device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(ct.device)\n",
    "            targets = targets.to(ct.device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / len(test_dataset)\n",
    "    print(f'Epoch {epoch+1}, Test Loss: {test_loss / len(test_dataloader)}, Test Acc: {accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model\n",
    "model.save('trained_model.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLModel' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ... (define the model architecture as before)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):  \u001b[38;5;66;03m# train for 30 epochs\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLModel' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ... (define the model architecture as before)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):  # train for 30 epochs\n",
    "    for batch in train_dataloader:\n",
    "        # Get the input and target tensors\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / len(test_dataset)\n",
    "    print(f'Epoch {epoch+1}, Test Loss: {test_loss / len(test_dataloader)}, Test Acc: {accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # Get the input and target tensors\n",
    "    inputs, targets = batch\n",
    "    print(np.shape(inputs),np.shape(targets))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3423\n",
      "Accuracy on test set: 91.62%\n",
      "Epoch [2/10], Loss: 0.2566\n",
      "Accuracy on test set: 92.40%\n",
      "Epoch [3/10], Loss: 0.2305\n",
      "Accuracy on test set: 92.94%\n",
      "Epoch [4/10], Loss: 0.2130\n",
      "Accuracy on test set: 92.88%\n",
      "Epoch [5/10], Loss: 0.2006\n",
      "Accuracy on test set: 93.00%\n",
      "Epoch [6/10], Loss: 0.1885\n",
      "Accuracy on test set: 92.99%\n",
      "Epoch [7/10], Loss: 0.1796\n",
      "Accuracy on test set: 93.10%\n",
      "Epoch [8/10], Loss: 0.1719\n",
      "Accuracy on test set: 93.14%\n",
      "Epoch [9/10], Loss: 0.1644\n",
      "Accuracy on test set: 92.90%\n",
      "Epoch [10/10], Loss: 0.1581\n",
      "Accuracy on test set: 93.02%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define the PyTorch model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128*1*1, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.bn1(self.relu1(self.conv1(x))))\n",
    "        x = self.pool2(self.bn2(self.relu2(self.conv2(x))))\n",
    "        x = self.pool3(self.bn3(self.relu3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = self.bn4(self.relu4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = len(class_dict)\n",
    "model = ConvNet(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "# # Convert to Core ML model\n",
    "# import coremltools as ct\n",
    "\n",
    "# # Trace the model with dummy input\n",
    "# dummy_input = torch.randn(1, 1, 28, 28)\n",
    "# traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# # Convert to Core ML\n",
    "# mlmodel = ct.convert(\n",
    "#     traced_model,\n",
    "#     inputs=[ct.TensorType(shape=dummy_input.shape)],\n",
    "#     classifier_config=ct.ClassifierConfig(class_dict)\n",
    "# )\n",
    "\n",
    "# # Save the Core ML model\n",
    "# mlmodel.save('trained_model.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (relu4): ReLU()\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet( num_classes=len(class_dict))\n",
    "\n",
    "# Check if CUDA is available and move the model to GPU\n",
    "device = 'cpu'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('C:/test001/ttt/cnn_classifier/trained_model.pth', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 82/83 [00:00<00:00, 4585.59 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<?, ? passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 963.18 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<?, ? passes/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BlobWriter not loaded",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mtrace(model, dummy_input)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert to Core ML\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m mlmodel \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraced_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClassifierConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Save the Core ML model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m mlmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjj_trained_model.mlmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\_converters_entry.py:581\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[0;32m    573\u001b[0m     specification_version \u001b[38;5;241m=\u001b[39m _set_default_specification_version(exact_target)\n\u001b[0;32m    575\u001b[0m use_default_fp16_io \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    576\u001b[0m     specification_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m specification_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m AvailableTarget\u001b[38;5;241m.\u001b[39miOS16\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m need_fp16_cast_pass\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m mlmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[0;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_model_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecification_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmain_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_fp16_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exact_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlprogram\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mlmodel\u001b[38;5;241m.\u001b[39m_input_has_infinite_upper_bound():\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor mlprogram, inputs with infinite upper_bound is not allowed. Please set upper_bound\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to a positive value in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRangeDim()\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m param in ct.convert().\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    601\u001b[0m     )\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\converter.py:188\u001b[0m, in \u001b[0;36mmil_convert\u001b[1;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;129m@_profile\u001b[39m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmil_convert\u001b[39m(\n\u001b[0;32m    151\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    156\u001b[0m ):\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    converter backend `convert_to`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_mil_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConverterRegistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMLModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\converter.py:212\u001b[0m, in \u001b[0;36m_mil_convert\u001b[1;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     weights_dir \u001b[38;5;241m=\u001b[39m _tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory()\n\u001b[0;32m    210\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_dir\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m--> 212\u001b[0m proto, mil_program \u001b[38;5;241m=\u001b[39m \u001b[43mmil_convert_to_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconvert_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m _reset_conversion_state()\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\converter.py:307\u001b[0m, in \u001b[0;36mmil_convert_to_proto\u001b[1;34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackend converter \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconvert_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not implemented, must be \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(converter_registry\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m backend_converter \u001b[38;5;241m=\u001b[39m backend_converter_type()\n\u001b[1;32m--> 307\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, prog\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\converter.py:130\u001b[0m, in \u001b[0;36mMILProtoBackend.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m backend_load\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\backend\\mil\\load.py:902\u001b[0m, in \u001b[0;36mload\u001b[1;34m(prog, weights_dir, resume_on_errors, specification_version, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;66;03m# convert pymil program into mil proto\u001b[39;00m\n\u001b[0;32m    898\u001b[0m mil_proto_exporter \u001b[38;5;241m=\u001b[39m MILProtoExporter(\n\u001b[0;32m    899\u001b[0m     prog,\n\u001b[0;32m    900\u001b[0m     weights_dir,\n\u001b[0;32m    901\u001b[0m )\n\u001b[1;32m--> 902\u001b[0m mil_proto \u001b[38;5;241m=\u001b[39m \u001b[43mmil_proto_exporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecification_version\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;66;03m# return the model provided by users\u001b[39;00m\n\u001b[0;32m    905\u001b[0m desc \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_description\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\test001\\ttt\\Lib\\site-packages\\coremltools\\converters\\mil\\backend\\mil\\load.py:400\u001b[0m, in \u001b[0;36mMILProtoExporter.export\u001b[1;34m(self, specification_version)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03mExport a pymil program into mil proto with the given specification version.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m BlobWriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlobWriter not loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    402\u001b[0m function_protos \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func_name, func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog\u001b[38;5;241m.\u001b[39mfunctions\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: BlobWriter not loaded"
     ]
    }
   ],
   "source": [
    "# Convert to Core ML model\n",
    "import coremltools as ct\n",
    "\n",
    "class_labels = list(class_dict.values())\n",
    "# Trace the model with dummy input\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# Convert to Core ML\n",
    "mlmodel = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.TensorType(shape=dummy_input.shape)],\n",
    "    classifier_config=ct.ClassifierConfig(class_labels)\n",
    ")\n",
    "\n",
    "# Save the Core ML model\n",
    "mlmodel.save('jj_trained_model.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113862, 784)\n",
      "Saved 10000 images to C:/test001/ttt/data/bed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bed.npy')\n",
    "\n",
    "# Check the shape of the data\n",
    "print(np.shape(array_data))  # Should output (126527, 784)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = 'C:/test001/ttt/data/bed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the number of images to save\n",
    "num_images_to_save = 10000\n",
    "\n",
    "# Save images\n",
    "for idx in range(min(num_images_to_save, len(array_data))):\n",
    "    img = array_data[idx]\n",
    "    \n",
    "    # Reshape the image to 28x28 pixels\n",
    "    image_reshaped = img.reshape(28, 28)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_reshaped, cmap='gray')\n",
    "    plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "    \n",
    "    # Save the image\n",
    "    output_path = os.path.join(output_dir, f'image_{idx}.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Clear the plot to avoid overlap\n",
    "    plt.clf()\n",
    "\n",
    "print(f\"Saved {min(num_images_to_save, len(array_data))} images to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(num_images_to_save, len(array_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144722, 784)\n",
      "Saved 1000 images to C:/test001/ttt/data/apple_sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "\n",
    "# Check the shape of the data\n",
    "print(np.shape(array_data))  # Should output (126527, 784)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = 'C:/test001/ttt/data/apple_sample'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the number of images to save\n",
    "num_images_to_save = 1000\n",
    "\n",
    "# Randomly select indices\n",
    "np.random.seed(123)  # For reproducibility\n",
    "random_indices = np.random.choice(len(array_data), num_images_to_save, replace=False)\n",
    "\n",
    "# Save images\n",
    "for idx in random_indices:\n",
    "    img = array_data[idx]\n",
    "    \n",
    "    # Reshape the image to 28x28 pixels\n",
    "    image_reshaped = img.reshape(28, 28)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_reshaped, cmap='gray')\n",
    "    plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "    \n",
    "    # Save the image\n",
    "    output_path = os.path.join(output_dir, f'image_{idx}.png')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Clear the plot to avoid overlap\n",
    "    plt.clf()\n",
    "\n",
    "print(f\"Saved {num_images_to_save} images to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
