{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134801, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'C:/demo/classfier_cnn/data\\The Eiffel Tower.npy' is the correct path to your .npy file\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/The Eiffel Tower.npy')\n",
    "\n",
    "# Get the shape of the array\n",
    "shape_of_array = np.shape(array_data)\n",
    "\n",
    "print(shape_of_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[6].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[6].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (40000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch\n",
    "X_test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (40000, 1, 28, 28)\n",
      "X_test shape after reshaping: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (10000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (40000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rank = 9\n",
    "\n",
    "first_image = X_test[rank].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "print(first_image.shape)\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.title(class_dict[y_test[rank]])\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[1]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x00000268A7C44A70>, <torch.utils.data.dataloader.DataLoader object at 0x00000268A7CE0B30>)\n",
      "Length of train dataloader: 1250 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9, 7, 9, 1, 7, 4, 7, 6, 3, 1, 5, 9, 8, 2, 3, 8, 4, 1, 4, 9, 3, 4,\n",
       "        7, 5, 9, 8, 9, 7, 8, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([28, 28, 1])\n",
      "Label: 3, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(452)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "# print(label,'pp')\n",
    "plt.title(class_dict[label.item()])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([28, 28, 1]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([28, 28]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class doodle_class(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_0 = doodle_class(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_dict) # one for every class\n",
    ")\n",
    "model_0.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test001\\ttt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 1/10 [00:00<00:06,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.01623 | Test loss: 0.94903, Test acc: 69.68%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 2/10 [00:01<00:06,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90624 | Test loss: 0.95613, Test acc: 69.19%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 3/10 [00:02<00:05,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.88992 | Test loss: 0.95127, Test acc: 70.47%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 4/10 [00:02<00:04,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.88244 | Test loss: 0.92794, Test acc: 70.78%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 5/10 [00:03<00:03,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.87614 | Test loss: 0.94376, Test acc: 70.46%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 6/10 [00:04<00:02,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87389 | Test loss: 0.93913, Test acc: 70.86%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 7/10 [00:04<00:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87162 | Test loss: 0.94464, Test acc: 70.39%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 8/10 [00:05<00:01,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86889 | Test loss: 0.96840, Test acc: 70.28%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 9/10 [00:06<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86419 | Test loss: 0.97103, Test acc: 69.54%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:06<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.86321 | Test loss: 0.94318, Test acc: 70.62%\n",
      "\n",
      "Train time on cpu: 6.932 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 10\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
       "        [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
       "        [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 17 14:05:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   38C    P8              15W / 170W |    844MiB / 12288MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4528    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      6956    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7160    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      8328    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8896    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9508    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10004    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13524    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     13872    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     16228    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     16716    C+G   ...s\\mattermost-desktop\\Mattermost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'doodle_class',\n",
       " 'model_loss': 0.9431753158569336,\n",
       " 'model_acc': 70.61701277955271}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), 'mod01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = input_tensor.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 28, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape#[0]#[176]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = torch.tensor(dat, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = array_data[11].reshape(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAALJ0lEQVR4nO3cT4jVZd/H8evYNA1IiAVKhRRIhSQumtrWFEXQX2aRxhQYUovclBQlaREFRhujFv2jciNZs2kqyKgoWvSHsUVFi4LSLCwqpJCEQZBzL3r4PDzPfS/m+7udM+P0eq3Ph+u3OJ73/BZevX6/328A0FpbMt8PAMDCIQoAhCgAEKIAQIgCACEKAIQoABCiAEAMzfaDvV5vLp8DgDk2m/+r7E0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIGZ9IR7w7y688MLyZmxsrNNZu3btKm+OHTvW6Sz+ubwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAESv3+/3Z/XBXm+unwVOmFNOOaW8mZycLG/Gx8fLm5mZmfKmtdbWrFlT3hw8eLDTWSxOs/m596YAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQLgllUXp/vvvL2+eeOKJgWzee++98qa11pYuXVrerFq1qrzZt29feTM9PV3eMHhuSQWgRBQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCEeA3PjjTd22t1xxx3lzWWXXVbenH766eXNkiWL7++qWf4k/B979uwpb7Zs2VLetNbab7/91mmHC/EAKBIFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIIbm+wGYfytXrixvnn/++fLmpptuKm9aa+37778vbyYnJ8ubQ4cOlTc//vhjefPLL7+UN6219tNPP5U3hw8fLm/uvvvu8mbr1q3lTdeL7bpepMfseFMAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfiLTLXX399ebNr167yZmRkpLzZvHlzedNaa88991x50+/3O5212CxdurS8GRsbO/EP8h8sW7ZsIOdQ400BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIFyIt0DdeeednXbPPvtseTM9PV3e3HbbbeXN/v37yxv+1/DwcHkzOTlZ3oyOjpY34+Pj5c0bb7xR3jD3vCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEG5JHYDt27eXN48++mins6ampsqbiYmJ8mZmZqa84W8rV67stHvzzTfLm0suuaS82bhxY3nT5XvHwuRNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBciFe0bdu28uaxxx4rb1544YXyprXWNm/eXN4cP36801l0s3Pnzk67NWvWlDfXXXddefPOO++UNywe3hQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAotfv9/uz+mCvN9fPMnDr1q0rbz7//PPyZvfu3eXNpk2byhsGb9WqVeXNDz/80OmsRx55pLzpchkji9dsfu69KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE0Hw/wImyZEm9by+//HJ58+uvv5Y399xzT3nDyWHt2rXlTZfvamutvf322512UOFNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBYNLekrlmzprwZHR0tbzZu3FjeHDlypLzh5DAzMzOws1avXl3e3HzzzeXNjh07yhvf8cXDmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBALJoL8c4777yBnPPVV18N5BxODvv27StvDhw40OmsV199tbzp9XrlzcUXX1ze7N27t7x58sknyxvmnjcFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgFg0F+KdeeaZAznnr7/+Gsg5nBy6fB+uuOKKTme99dZb5c2KFSvKm6uvvrq8ueqqq8qbZcuWlTettXbaaaeVN9u3by9vjh8/Xt4sBt4UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAKLX7/f7s/pgrzfXz/JfOffcc8ubAwcOlDdbtmwpb5566qnyBk6E5cuXlzdnnHFGeTM1NVXerF27trzpasOGDeXN5OTkHDzJ/JrNz703BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBYNBfidfHJJ5+UN10uC1u3bl15c+zYsfIG5kuXy+3Wr1/f6awuu59//rm8ufLKK8ubhc6FeACUiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBA/KNvSb388svLmw8//LC82bZtW3nz+OOPlzfwT/DAAw+UN13+PXW5EfnPP/8sbwbJLakAlIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEEPz/QDz6aOPPipvdu/eXd489NBD5c3evXvLm9Za++KLLzrt4GTx5ZdfljddLvS84IILypvp6enyZqHxpgBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQvX6/35/VBztcKLUYLV++vLzpcknWqaeeWt601tqll15a3vz++++dzoL5sHr16vLmu+++K28mJibKmz179pQ3gzSbn3tvCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxNN8PcLL5448/ypvx8fHy5tNPPy1vWmvt448/Lm9uuOGG8ubbb78tb+BEOHr06EDOGRkZGcg5C403BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwId4AfP311+XN2NhYp7OmpqbKm88++6y82bBhQ3nz7rvvljfw/5199tkDOefIkSMDOWeh8aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQPT6/X5/Vh/s9eb6WTgBzjrrrPLm9ddfL29GR0fLm3vvvbe8aa21p59+utOOha3rbaevvfZaeXPRRReVN+eff355c/jw4fJmkGbzc+9NAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBciEcbGRkpb1566aXyZmJiorxprbVXXnmlvHnwwQfLm4MHD5Y3/O3WW28tb7pedDg0NFTe3H777eVNl4siFzoX4gFQIgoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBCPTrp8H+67775OZz388MPlzfDwcHnz4osvljc7duwobw4dOlTetNbaihUryptrr722vLnlllvKm2uuuaa8+eCDD8qb1lrbtGlTeeOyw7+5EA+AElEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAwoV4LHhdLoLbunVreXPXXXeVNyMjI+XNQrd///7yZufOneXNM888U960NrtL3fjPXIgHQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIRbUuF/nHPOOeXN+vXry5vh4eHyprXWjh49Wt68//775c0333xT3nBycEsqACWiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQL8QD+IVyIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxNNsPzvLePABOYt4UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiH8ByxrF0YzRzqUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[66].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_apple\n",
      "Predicted class index: 5\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model_0(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNNModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# PyTorch expects (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert labels to tensor\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data), labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume data and labels are already loaded as numpy arrays\n",
    "# data: shape (num_samples, 28, 28)\n",
    "# labels: shape (num_samples,)\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype('float32') / 255.0\n",
    "data = data.reshape(-1, 1, 28, 28)  # PyTorch expects (batch_size, channels, height, width)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.from_numpy(labels).long()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(torch.from_numpy(data), labels)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader and print the shapes and some values\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch image shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch labels shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Dummy data for demonstration purposes\n",
    "num_samples = 10000\n",
    "data = np.random.rand(num_samples, 28, 28).astype('float32')\n",
    "labels = np.random.randint(0, 10, num_samples)\n",
    "\n",
    "# Define any transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader and print the shapes and some values\n",
    "for images, labels in data_loader:\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Sample image data: {images[0]}\")\n",
    "    print(f\"Sample label: {labels[0]}\")\n",
    "    break  # Just to print the first batch and stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the inputs and labels\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a custom dataset class for our data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define a CNN model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(30*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 30*4*4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create a dataset and data loader for our data\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = Net()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss at each batch\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
