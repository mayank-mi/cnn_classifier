{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy')\n",
    "\n",
    "# Extract the first image\n",
    "first_image = array_data[16].reshape(28, 28)  # Assuming each image is 28x28 pixels (adjust as per your data)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_train shape: (40000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# List of file paths for the 10 classes\n",
    "file_paths = [\n",
    "    'C:/demo/classfier_cnn/data/The Eiffel Tower.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_basketball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_baseball.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bathtub.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bicycle.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_apple.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_bat.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_alarm clock.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy',\n",
    "    'C:/demo/classfier_cnn/data/full_numpy_bitmap_book.npy'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(file_paths, num_samples=5000):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        array_data = np.load(file_path)\n",
    "        # Get the first num_samples samples\n",
    "        first_samples = array_data[:num_samples]\n",
    "        data.append(first_samples)\n",
    "        # Create labels for these samples\n",
    "        labels.append(np.full((num_samples,), idx))\n",
    "    return np.vstack(data), np.concatenate(labels)\n",
    "\n",
    "# Load the data\n",
    "data, labels = load_data(file_paths)\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data if necessary (e.g., if the images are 28x28 pixels)\n",
    "data = data.reshape(-1, 28, 28, 1)  # Assuming the images are 28x28 pixels and grayscale\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "y_test\n",
    "unique_elements = np.unique(y_test)\n",
    "print(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'The Eiffel Tower', 1: 'full_numpy_bitmap_basketball', 2: 'full_numpy_bitmap_baseball', 3: 'full_numpy_bitmap_bathtub', 4: 'full_numpy_bitmap_bicycle', 5: 'full_numpy_bitmap_apple', 6: 'full_numpy_bitmap_bat', 7: 'full_numpy_bitmap_alarm clock', 8: 'full_numpy_bitmap_airplane', 9: 'full_numpy_bitmap_book'}\n"
     ]
    }
   ],
   "source": [
    "# Define the class labels\n",
    "class_labels = [\n",
    "    'The Eiffel Tower',\n",
    "    'full_numpy_bitmap_basketball',\n",
    "    'full_numpy_bitmap_baseball',\n",
    "    'full_numpy_bitmap_bathtub',\n",
    "    'full_numpy_bitmap_bicycle',\n",
    "    'full_numpy_bitmap_apple',\n",
    "    'full_numpy_bitmap_bat',\n",
    "    'full_numpy_bitmap_alarm clock',\n",
    "    'full_numpy_bitmap_airplane',\n",
    "    'full_numpy_bitmap_book'\n",
    "]\n",
    "\n",
    "# Create the dictionary\n",
    "class_dict = {i: label for i, label in enumerate(class_labels)}\n",
    "\n",
    "# Print the dictionary\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after reshaping: (40000, 1, 28, 28)\n",
      "X_test shape after reshaping: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test, (10000, 1, 28, 28))\n",
    "X_train_reshaped = np.reshape(X_train, (40000, 1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_train shape after reshaping:\", X_train_reshaped.shape)\n",
    "print(\"X_test shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test[9] shape after reshaping: (1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_test_reshaped = np.reshape(X_test[9], (1, 28, 28))\n",
    "\n",
    "# Print the shape after reshaping\n",
    "print(\"X_test[9] shape after reshaping:\", X_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are already defined as numpy arrays or tensors\n",
    "\n",
    "# Convert data to tensors if they are numpy arrays\n",
    "if isinstance(X_train_reshaped, np.ndarray):\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "if isinstance(X_test_reshaped, np.ndarray):\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "if isinstance(y_test, np.ndarray):\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Combine data and labels into TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_dataset, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "image_to_plot = X_train[1]  # Assuming you want to plot the second image (index 1)\n",
    "\n",
    "# Plot the image directly using PyTorch and matplotlib\n",
    "plt.imshow(image_to_plot.squeeze(0), cmap='gray')  # Squeeze along the channel dimension\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x000002DC87B10680>, <torch.utils.data.dataloader.DataLoader object at 0x000002DC08286750>)\n",
      "Length of train dataloader: 1250 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 28, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 0, 7, 1, 2, 5, 0, 8, 4, 6, 1, 3, 8, 3, 8, 6, 4, 9, 8, 9, 7,\n",
       "        7, 7, 3, 9, 3, 9, 8, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'full_numpy_bitmap_basketball'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([28, 28, 1])\n",
      "Label: 8, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(452)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(0), cmap=\"gray\")\n",
    "# print(label,'pp')\n",
    "plt.title(class_dict[label.item()])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([28, 28, 1]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([28, 28]) -> [color_channels, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class doodle_class_model_02(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class_model_02(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_2 = doodle_class_model_02(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_dict) # one for every class\n",
    ")\n",
    "model_2.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 28, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:21,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.70738 | Test loss: 1.33266, Test acc: 59.82%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:20,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.19682 | Test loss: 1.15681, Test acc: 63.11%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.09591 | Test loss: 1.10516, Test acc: 64.55%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:17,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.05320 | Test loss: 1.07620, Test acc: 64.85%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:17,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 1.02609 | Test loss: 1.05690, Test acc: 65.37%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:04<00:17,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.00734 | Test loss: 1.04347, Test acc: 65.94%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.99291 | Test loss: 1.03770, Test acc: 66.29%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.98093 | Test loss: 1.02566, Test acc: 66.41%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:06<00:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.97024 | Test loss: 1.02264, Test acc: 66.08%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:07<00:14,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.96078 | Test loss: 1.01385, Test acc: 66.47%\n",
      "\n",
      "Epoch: 10\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:07<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.95356 | Test loss: 1.00486, Test acc: 66.98%\n",
      "\n",
      "Epoch: 11\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:08<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.94648 | Test loss: 0.99812, Test acc: 67.45%\n",
      "\n",
      "Epoch: 12\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:09<00:12,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.93951 | Test loss: 0.99558, Test acc: 67.56%\n",
      "\n",
      "Epoch: 13\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:10<00:11,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.93393 | Test loss: 0.99126, Test acc: 67.10%\n",
      "\n",
      "Epoch: 14\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:10<00:11,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.92851 | Test loss: 0.98647, Test acc: 67.77%\n",
      "\n",
      "Epoch: 15\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:11<00:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.92348 | Test loss: 0.98517, Test acc: 67.83%\n",
      "\n",
      "Epoch: 16\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:12<00:09,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.91871 | Test loss: 0.98219, Test acc: 67.73%\n",
      "\n",
      "Epoch: 17\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:13<00:08,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.91354 | Test loss: 0.98399, Test acc: 67.65%\n",
      "\n",
      "Epoch: 18\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:13<00:08,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.90948 | Test loss: 0.97802, Test acc: 68.06%\n",
      "\n",
      "Epoch: 19\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:14<00:07,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90528 | Test loss: 0.97421, Test acc: 67.94%\n",
      "\n",
      "Epoch: 20\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:15<00:06,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.90135 | Test loss: 0.97211, Test acc: 68.03%\n",
      "\n",
      "Epoch: 21\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:15<00:05,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.89732 | Test loss: 0.96646, Test acc: 68.23%\n",
      "\n",
      "Epoch: 22\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:16<00:05,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.89328 | Test loss: 0.97419, Test acc: 67.83%\n",
      "\n",
      "Epoch: 23\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:17<00:04,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88941 | Test loss: 0.96988, Test acc: 67.93%\n",
      "\n",
      "Epoch: 24\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:18<00:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.88651 | Test loss: 0.97250, Test acc: 67.30%\n",
      "\n",
      "Epoch: 25\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:18<00:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.88350 | Test loss: 0.95762, Test acc: 68.65%\n",
      "\n",
      "Epoch: 26\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:19<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.87968 | Test loss: 0.97616, Test acc: 67.29%\n",
      "\n",
      "Epoch: 27\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:20<00:01,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87655 | Test loss: 0.95403, Test acc: 68.66%\n",
      "\n",
      "Epoch: 28\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n",
      "Looked at 38400/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:21<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.87328 | Test loss: 0.95392, Test acc: 68.40%\n",
      "\n",
      "Epoch: 29\n",
      "-------\n",
      "Looked at 0/40000 samples\n",
      "Looked at 12800/40000 samples\n",
      "Looked at 25600/40000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:21<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 38400/40000 samples\n",
      "\n",
      "Train loss: 0.87060 | Test loss: 0.95269, Test acc: 68.38%\n",
      "\n",
      "Train time on cpu: 21.941 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 30\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_2.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_2(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_2(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_2.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
       "        [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
       "        [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'doodle_class_model_02',\n",
       " 'model_loss': 0.9526905417442322,\n",
       " 'model_acc': 68.3805910543131}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:03<01:27,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.0201 | Test Loss: 0.7484 | Test Acc: 76.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:06<01:24,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.8631 | Test Loss: 0.6945 | Test Acc: 78.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:09<01:23,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.8121 | Test Loss: 0.6578 | Test Acc: 78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:12<01:19,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.7771 | Test Loss: 0.6329 | Test Acc: 79.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:15<01:17,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.7551 | Test Loss: 0.6193 | Test Acc: 79.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:18<01:15,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.7274 | Test Loss: 0.6002 | Test Acc: 80.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:21<01:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.7086 | Test Loss: 0.5871 | Test Acc: 81.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:24<01:09,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.7037 | Test Loss: 0.5855 | Test Acc: 81.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:28<01:06,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.6903 | Test Loss: 0.5785 | Test Acc: 81.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:31<01:03,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.6754 | Test Loss: 0.5707 | Test Acc: 81.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:34<01:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.6367 | Test Loss: 0.5565 | Test Acc: 82.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:37<00:56,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.6238 | Test Loss: 0.5548 | Test Acc: 82.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:40<00:53,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.6236 | Test Loss: 0.5492 | Test Acc: 82.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:43<00:49,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.6188 | Test Loss: 0.5441 | Test Acc: 82.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:46<00:46,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.6094 | Test Loss: 0.5413 | Test Acc: 82.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:49<00:43,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.6064 | Test Loss: 0.5413 | Test Acc: 82.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:53<00:40,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.6048 | Test Loss: 0.5419 | Test Acc: 82.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:56<00:37,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.6000 | Test Loss: 0.5379 | Test Acc: 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:59<00:34,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.5948 | Test Loss: 0.5361 | Test Acc: 82.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [01:02<00:31,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.5984 | Test Loss: 0.5346 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [01:05<00:27,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.5859 | Test Loss: 0.5369 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [01:08<00:25,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.5886 | Test Loss: 0.5356 | Test Acc: 82.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [01:11<00:22,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.5884 | Test Loss: 0.5368 | Test Acc: 82.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [01:15<00:18,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.5846 | Test Loss: 0.5360 | Test Acc: 82.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [01:18<00:15,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.5860 | Test Loss: 0.5358 | Test Acc: 82.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [01:21<00:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.5882 | Test Loss: 0.5338 | Test Acc: 82.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [01:24<00:09,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.5889 | Test Loss: 0.5353 | Test Acc: 82.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [01:27<00:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.5877 | Test Loss: 0.5350 | Test Acc: 82.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [01:30<00:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.5860 | Test Loss: 0.5356 | Test Acc: 82.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:33<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.5833 | Test Loss: 0.5328 | Test Acc: 82.75%\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DoodleClassModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units*2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_units*2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Define accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate the model\n",
    "model_2 = DoodleClassModel(input_shape=784, hidden_units=128, output_shape=len(class_dict))\n",
    "model_2.to(\"cpu\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Data loading (assuming train_dataloader and test_dataloader are defined)\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model_2.train()\n",
    "    train_loss = 0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model_2(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model_2.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "            test_pred = model_2(X)\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DoodleClassModel',\n",
       " 'model_loss': 0.532804548740387,\n",
       " 'model_acc': 82.74760383386581}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_2, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
       "        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
       "        [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
       "        ...,\n",
       "        [-0.0195,  0.0034,  0.0302,  ..., -0.0030, -0.0317,  0.0128],\n",
       "        [-0.0107,  0.0221, -0.0158,  ..., -0.0121,  0.0042,  0.0318],\n",
       "        [-0.0106,  0.0342,  0.0240,  ...,  0.0091,  0.0174,  0.0041]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doodle_class(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_0.state_dict(), 'mod01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the sample image\n",
    "# sample_image_path = 'path_to_your_sample_image.jpg'  # Replace with your actual image path\n",
    "# sample_image = Image.open(sample_image_path)\n",
    "\n",
    "array_data = np.load('C:/demo/classfier_cnn/data/full_numpy_bitmap_airplane.npy')\n",
    "# Extract the first image\n",
    "input_tensor = array_data[6].reshape(28, 28,1)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(input_tensor, cmap='gray')\n",
    "plt.axis('off')  # Optional: turn off axis numbers and ticks\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class label: full_numpy_bitmap_airplane\n",
      "Predicted class index: 8\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sample image\n",
    "sample_image_tensor = preprocess_sample_image(input_tensor)\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "with torch.no_grad():\n",
    "    output = model_2(sample_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Print the predicted class\n",
    "\n",
    "print(f\"Predicted class label: {class_dict[predicted_class]}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample_image(image_array):\n",
    "    # Normalize the image\n",
    "    image_array = image_array.astype('float32') / 255.0\n",
    "    # Reshape the image to (1, 28, 28, 1) if it's a single image\n",
    "    image_array = np.reshape(image_array, (1, 1, 28, 28))\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "    # Flatten the image tensor to shape (1, 784)\n",
    "    image_tensor = image_tensor.view(1, -1)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNNModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# PyTorch expects (batch_size, channels, height, width)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert labels to tensor\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data), labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume data and labels are already loaded as numpy arrays\n",
    "# data: shape (num_samples, 28, 28)\n",
    "# labels: shape (num_samples,)\n",
    "\n",
    "# Normalize data\n",
    "data = data.astype('float32') / 255.0\n",
    "data = data.reshape(-1, 1, 28, 28)  # PyTorch expects (batch_size, channels, height, width)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.from_numpy(labels).long()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(torch.from_numpy(data), labels)\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader and print the shapes and some values\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch image shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBatch labels shape: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Dummy data for demonstration purposes\n",
    "num_samples = 10000\n",
    "data = np.random.rand(num_samples, 28, 28).astype('float32')\n",
    "labels = np.random.randint(0, 10, num_samples)\n",
    "\n",
    "# Define any transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate over the DataLoader and print the shapes and some values\n",
    "for images, labels in data_loader:\n",
    "    print(f\"Batch image shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Sample image data: {images[0]}\")\n",
    "    print(f\"Sample label: {labels[0]}\")\n",
    "    break  # Just to print the first batch and stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the inputs and labels\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a custom dataset class for our data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define a CNN model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(30*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv2(x), 2))\n",
    "        x = torch.relu(torch.max_pool2d(self.conv3(x), 2))\n",
    "        x = x.view(-1, 30*4*4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create a dataset and data loader for our data\n",
    "dataset = ImageDataset(data, labels, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = Net()\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss at each batch\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get the inputs and labels\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\demo\\classfier_cnn\\Lib\\site-packages\\torchvision\\transforms\\functional.py:154\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    152\u001b[0m     pic \u001b[38;5;241m=\u001b[39m pic[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 154\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# backward compatibility\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        images, labels = images.to(device), labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classfier_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
